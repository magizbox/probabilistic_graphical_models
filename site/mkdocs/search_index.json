{
    "docs": [
        {
            "location": "/", 
            "text": "Probabilistic Graphical Models\n\n\n\n\nProbabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.\n\n\n\n\nTable of Contents\n\n\nThis site is intended to host a variety of resources and pointers to information about Probabilistic Graphical Models. \n\n\nRepresentation\n\n\n\n\nIntroduction\n\n\nBayesian Network (Directed Models)\n\n\nTemplate Models for Bayesian Networks\n\n\nStructured CPDs for Bayesian Networks\n\n\nMarkov Networks (Undirected Models)\n\n\nDecision Making\n\n\nReal world examples\n\n\n\n\nInference\n\n\n\n\nInference Overview\n\n\nBelief Propagation Algorithms\n\n\nMAP Algorithms\n\n\nSampling Methods\n\n\nInference in Temporal Models\n\n\n\n\nLearning\n\n\n\n\nLearning Overview\n\n\nParameter Estimation in Bayesian Networks\n\n\nLearning Undirected Models\n\n\nLearning BN Structure\n\n\nLearning BNs with Incomplete Data\n\n\nLearning Summary\n\n\n\n\nMiscellaneous\n\n\n\n\nBooks\n\n\nCourses\n\n\nSoftwares\n\n\nDatasets\n\n\n\n\nSoftwares\n\n\n\n\nAn introduction to UnBBayes\n\n\n\n\nDatasets\n\n\n\n\nMedical Domain Data\n\n\nOptical Word Recognition\n\n\n\n\nBooks\n\n\n\n\n\n\n\n\nCourses", 
            "title": "Home"
        }, 
        {
            "location": "/#probabilistic-graphical-models", 
            "text": "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.", 
            "title": "Probabilistic Graphical Models"
        }, 
        {
            "location": "/#table-of-contents", 
            "text": "This site is intended to host a variety of resources and pointers to information about Probabilistic Graphical Models.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/#representation", 
            "text": "Introduction  Bayesian Network (Directed Models)  Template Models for Bayesian Networks  Structured CPDs for Bayesian Networks  Markov Networks (Undirected Models)  Decision Making  Real world examples", 
            "title": "Representation"
        }, 
        {
            "location": "/#inference", 
            "text": "Inference Overview  Belief Propagation Algorithms  MAP Algorithms  Sampling Methods  Inference in Temporal Models", 
            "title": "Inference"
        }, 
        {
            "location": "/#learning", 
            "text": "Learning Overview  Parameter Estimation in Bayesian Networks  Learning Undirected Models  Learning BN Structure  Learning BNs with Incomplete Data  Learning Summary", 
            "title": "Learning"
        }, 
        {
            "location": "/#miscellaneous", 
            "text": "Books  Courses  Softwares  Datasets", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/#softwares", 
            "text": "An introduction to UnBBayes", 
            "title": "Softwares"
        }, 
        {
            "location": "/#datasets", 
            "text": "Medical Domain Data  Optical Word Recognition", 
            "title": "Datasets"
        }, 
        {
            "location": "/#books", 
            "text": "", 
            "title": "Books"
        }, 
        {
            "location": "/#courses", 
            "text": "", 
            "title": "Courses"
        }, 
        {
            "location": "/representation/", 
            "text": "Representation\n\n\nProbabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other.\n\n\nThese representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.", 
            "title": "Introduction"
        }, 
        {
            "location": "/representation/#representation", 
            "text": "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other.  These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.", 
            "title": "Representation"
        }, 
        {
            "location": "/foundation_probability/", 
            "text": "Foundation: Probability Theory\n\n\nThe main focus of this book is on complex probability distributions. In this section we briefly\nreview basic concepts from probability theory.\n\n\n1 Probability Distributions\n\n\nWhen we use the word \u201cprobability\u201d in day-to-day life, we refer to a degree of confidence that\nan event of an uncertain nature will occur. For example, the weather report might say \u201cthere\nis a low probability of light rain in the afternoon.\u201d Probability theory deals with the formal\nfoundations for discussing such estimates and the rules they should obey.\nBefore we discuss the representation of probability, we need to define what the events are to\nwhich we want to assign a probability. These events might be di\u001berent outcomes of throwing\na die, the outcome of a horse race, the weather configurations in California, or the possible\nfailures of a piece of machinery.\n\n\n1.1 Event Spaces\n\n\nevent Formally, we define events by assuming that there is an agreed upon space of possible outcomes,\noutcome space which we denote by \u2126. For example, if we consider dice, we might set \u2126 = {1, 2, 3, 4, 5, 6}. In\nthe case of a horse race, the space might be all possible orders of arrivals at the finish line, a\nmuch larger space.\n\n\nmeasurable event In addition, we assume that there is a set of measurable events S to which we are willing to\nassign probabilities. Formally, each event \u03b1 \u2208 S is a subset of \u2126. In our die example, the event\n{6} represents the case where the die shows 6, and the event {1, 3, 5} represents the case of\nan odd outcome. In the horse-race example, we might consider the event \u201cLucky Strike wins,\u201d\nwhich contains all the outcomes in which the horse Lucky Strike is first.\nProbability theory requires that the event space satisfy three basic properties:\n\u2022 It contains the empty event \u2205, and the trivial event \u2126.\n\u2022 It is closed under union. That is, if \u03b1, \u03b2 \u2208 S, then so is \u03b1 \u222a \u03b2.\n\u2022 It is closed under complementation. That is, if \u03b1 \u2208 S, then so is \u2126 \u2212 \u03b1.\nThe requirement that the event space is closed under union and complementation implies that\nit is also closed under other Boolean operations, such as intersection and set di\u001berence.\n\n\n1.2 Probability Distributions\n\n\nDefinition 2.1 A probability distribution P over (\u2126, S) is a mapping from events in S to real values that satisfies\nprobability\ndistribution\nthe following conditions:\n\u2022 P(\u03b1) \u2265 0 for all \u03b1 \u2208 S.\n\u2022 P(\u2126) = 1.\n\u2022 If \u03b1, \u03b2 \u2208 S and \u03b1 \u2229 \u03b2 = \u2205, then P(\u03b1 \u222a \u03b2) = P(\u03b1) + P(\u03b2).\nThe first condition states that probabilities are not negative. The second states that the \u201ctrivial\nevent,\u201d which allows all possible outcomes, has the maximal possible probability of 1. The third\ncondition states that the probability that one of two mutually disjoint events will occur is the\nsum of the probabilities of each event. These two conditions imply many other conditions. Of\nparticular interest are P(\u2205) = 0, and P(\u03b1 \u222a \u03b2) = P(\u03b1) + P(\u03b2) \u2212 P(\u03b1 \u2229 \u03b2).\n\n\n1.3 Interpretations of Probability\n\n\nBefore we continue to discuss probability distributions, we need to consider the interpretations\nthat we might assign to them. Intuitively, the probability P(\u03b1) of an event \u03b1 quantifies the\ndegree of confidence that \u03b1 will occur. If P(\u03b1) = 1, we are certain that one of the outcomes\nin \u03b1 occurs, and if P(\u03b1) = 0, we consider all of them impossible. Other probability values\nrepresent options that lie between these two extremes.\nThis description, however, does not provide an answer to what the numbers mean. There are\ntwo common interpretations for probabilities.\nfrequentist The frequentist interpretation views probabilities as frequencies of events. More precisely, the\ninterpretation probability of an event is the fraction of times the event occurs if we repeat the experiment\nindefinitely. For example, suppose we consider the outcome of a particular die roll. In this case,\nthe statement P(\u03b1) = 0.3, for \u03b1 = {1, 3, 5}, states that if we repeatedly roll this die and record\nthe outcome, then the fraction of times the outcomes in \u03b1 will occur is 0.3. More precisely, the\nlimit of the sequence of fractions of outcomes in \u03b1 in the first roll, the first two rolls, the first\nthree rolls, . . ., the first n rolls, . . . is 0.3.\n\n\nThe frequentist interpretation gives probabilities a tangible semantics. When we discuss\nconcrete physical systems (for example, dice, coin flips, and card games) we can envision how\nthese frequencies are defined. It is also relatively straightforward to check that frequencies must\nsatisfy the requirements of proper distributions.\nThe frequentist interpretation fails, however, when we consider events such as \u201cIt will rain\ntomorrow afternoon.\u201d Although the time span of \u201cTomorrow afternoon\u201d is somewhat ill defined,\nwe expect it to occur exactly once. It is not clear how we define the frequencies of such events.\nSeveral attempts have been made to define the probability for such an event by finding a\nreference class reference class of similar events for which frequencies are well defined; however, none of them\nhas proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory\ninterpretation for a statement such as \u201cthe probability of rain tomorrow afternoon is 0.3.\u201d\n\u0011 An alternative interpretation views probabilities as subjective degrees of belief. Under\nsubjective\ninterpretation\nthis interpretation, the statement P(\u03b1) = 0.3 represents a subjective statement about\none\u2019s own degree of belief that the event \u03b1 will come about. Thus, the statement \u201cthe\nprobability of rain tomorrow afternoon is 50 percent\u201d tells us that in the opinion of the speaker,\nthe chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon\nwill occur only once, we can still have uncertainty about its outcome, and represent it using\nnumbers (that is, probabilities).\nThis description still does not resolve what exactly it means to hold a particular degree of\nbelief. What stops a person from stating that the probability that Bush will win the election\nis 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need\nto explain how subjective degrees of beliefs (something that is internal to each one of us) are\nreflected in our actions.\nThis issue is a major concern in subjective probabilities. One possible way of attributing\ndegrees of beliefs is by a betting game. Suppose you believe that P(\u03b1) = 0.8. Then you would\nbe willing to place a bet of $1 against $3. To see this, note that with probability 0.8 you gain a\ndollar, and with probability 0.2 you lose $3, so on average this bet is a good deal with expected\ngain of 20 cents. In fact, you might be even tempted to place a bet of $1 against $4. Under\nthis bet the average gain is 0, so you should not mind. However, you would not consider it\nworthwhile to place a bet $1 against $4 and 10 cents, since that would have negative expected\ngain. Thus, by finding which bets you are willing to place, we can assess your degrees of beliefs.\nThe key point of this mental game is the following. If you hold degrees of belief that do not\nsatisfy the rule of probability, then by a clever construction we can find a series of bets that\nwould result in a sure negative outcome for you. Thus, the argument goes, a rational person\nmust hold degrees of belief that satisfy the rules of probability.1\nIn the remainder of the book we discuss probabilities, but we usually do not explicitly state\ntheir interpretation. Since both interpretations lead to the same mathematical rules, the technical\ndefinitions hold for both interpretations.\n\n\n2 Basic Concepts in Probability\n\n\n2.1 Conditional Probability\n\n\nTo use a concrete example, suppose we consider a distribution over a population of students\ntaking a certain course. The space of outcomes is simply the set of all students in the population.\nNow, suppose that we want to reason about the students\u2019 intelligence and their final grade. We\ncan define the event \u03b1 to denote \u201call students with grade A,\u201d and the event \u03b2 to denote \u201call\nstudents with high intelligence.\u201d Using our distribution, we can consider the probability of these\nevents, as well as the probability of \u03b1 \u2229 \u03b2 (the set of intelligent students who got grade A). This,\nhowever, does not directly tell us how to update our beliefs given new evidence. Suppose we\nlearn that a student has received the grade A; what does that tell us about her intelligence?\nThis kind of question arises every time we want to use distributions to reason about the real\nworld. More precisely, after learning that an event \u03b1 is true, how do we change our probability\nconditional about \u03b2 occurring? The answer is via the notion of conditional probability. Formally, the\nprobability conditional probability of \u03b2 given \u03b1 is defined as\nP(\u03b2 | \u03b1) = P(\u03b1 \u2229 \u03b2)\nP(\u03b1) (2.1)\nThat is, the probability that \u03b2 is true given that we know \u03b1 is the relative proportion of outcomes\nsatisfying \u03b2 among these that satisfy \u03b1. (Note that the conditional probability is not defined\nwhen P(\u03b1) = 0.)\nThe conditional probability given an event (say \u03b1) satisfies the properties of definition 2.1 (see\nexercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think\nof the conditioning operation as taking one distribution and returning another over the same\nprobability space.\n\n\n2.2 Chain Rule and Bayes Rule\n\n\nFrom the definition of the conditional distribution, we immediately see that\nP(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2 | \u03b1). (2.2)\nchain rule This equality is known as the chain rule of conditional probabilities. More generally, if\n\u03b11, . . . , \u03b1k are events, then we can write\nP(\u03b11 \u2229 . . . \u2229 \u03b1k) = P(\u03b11)P(\u03b12 | \u03b11) \u00b7 \u00b7 \u00b7 P(\u03b1k | \u03b11 \u2229 . . . \u2229 \u03b1k\u22121). (2.3)\nIn other words, we can express the probability of a combination of several events in terms of the\nprobability of the first, the probability of the second given the first, and so on. It is important\nto notice that we can expand this expression using any order of events \u2014 the result will remain\nthe same.\nBayes\u2019 rule Another immediate consequence of the definition of conditional probability is Bayes\u2019 rule\nP(\u03b1 | \u03b2) = P(\u03b2 | \u03b1)P(\u03b1)\nP(\u03b2)\n\n\nA more general conditional version of Bayes\u2019 rule, where all our probabilities are conditioned on\nsome background event \u03b3, also holds:\nP(\u03b1 | \u03b2 \u2229 \u03b3) = P(\u03b2 | \u03b1 \u2229 \u03b3)P(\u03b1 | \u03b3)\nP(\u03b2 | \u03b3) .\nBayes\u2019 rule is important in that it allows us to compute the conditional probability P(\u03b1 | \u03b2)\nfrom the \u201cinverse\u201d conditional probability P(\u03b2 | \u03b1).\nExample 2.1 Consider the student population, and let Smart denote smart students and GradeA denote students who got grade A. Assume we believe (perhaps based on estimates from past statistics) that\nP(GradeA | Smart) = 0.6, and now we learn that a particular student received grade A. Can\nwe estimate the probability that the student is smart? According to Bayes\u2019 rule, this depends on\nprior our prior probability for students being smart (before we learn anything about them) and the\nprior probability of students receiving high grades. For example, suppose that P(Smart) = 0.3\nand P(GradeA) = 0.2, then we have that P(Smart | GradeA) = 0.6 \u2217 0.3/0.2 = 0.9. That\nis, an A grade strongly suggests that the student is smart. On the other hand, if the test was\neasier and high grades were more common, say, P(GradeA) = 0.4 then we would get that\nP(Smart | GradeA) = 0.6 \u2217 0.3/0.4 = 0.45, which is much less conclusive about the student.\nAnother classic example that shows the importance of this reasoning is in disease screening.\nTo see this, consider the following hypothetical example (none of the mentioned figures are\nrelated to real statistics).\nExample 2.2 Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected,\nthen the test will be positive with probability 0.95, and if the patient is not infected, then the test\nwill be negative with probability 0.95. Now suppose that a person gets a positive test result. What is\nthe probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent\nof the time, then the probability that the subject is infected is 0.95. That is, 95 percent of subjects\nwith positive results have TB.\nIf we consider the problem by applying Bayes\u2019 rule, we see that we need to consider the prior\nprobability of TB infection, and the probability of getting positive test result. Suppose that 1 in\n1000 of the subjects who get tested is infected. That is, P(TB) = 0.001. What is the probability of\ngetting a positive test result? From our description, we see that 0.001 \u00b7 0.95 infected subjects get a\npositive result, and 0.999\u00b70.05 uninfected subjects get a positive result. Thus, P(Positive) = 0.0509.\nApplying Bayes\u2019 rule, we get that P(TB | Positive) = 0.001\u00b70.95/0.0509 \u2248 0.0187. Thus, although\na subject with a positive test is much more probable to be TB-infected than is a random subject,\nfewer than 2 percent of these subjects are TB-infected.\n\n\n3 Random Variables and Joint Distributions\n\n\n3.1 Motivation\n\n\nOur discussion of probability distributions deals with events. Formally, we can consider any\nevent from the set of measurable events. The description of events is in terms of sets of\noutcomes. In many cases, however, it would be more natural to consider attributes of the\noutcome. For example, if we consider a patient, we might consider attributes such as \u201cage,\u201d\n\n\n\u201cgender,\u201d and \u201csmoking history\u201d that are relevant for assigning probability over possible diseases\nand symptoms. We would like then consider events such as \u201cage \n 55, heavy smoking history,\nand su\u001bers from repeated cough.\u201d\nTo use a concrete example, consider again a distribution over a population of students in a\ncourse. Suppose that we want to reason about the intelligence of students, their final grades,\nand so forth. We can use an event such as GradeA to denote the subset of students that received\nthe grade A and use it in our formulation. However, this discussion becomes rather cumbersome\nif we also want to consider students with grade B, students with grade C, and so on. Instead, we\nwould like to consider a way of directly referring to a student\u2019s grade in a clean, mathematical\nway.\nThe formal machinery for discussing attributes and their values in di\u001berent outcomes are\nrandom variable random variables. A random variable is a way of reporting an attribute of the outcome. For\nexample, suppose we have a random variable Grade that reports the final grade of a student,\nthen the statement P (Grade = A) is another notation for P (GradeA).\n\n\nn the statement P (Grade = A) is another notation for P (GradeA).\n\n\n3.2 What Is a Random Variable?\n\n\nFormally, a random variable, such as Grade, is defined by a function that associates with each\noutcome in \u2126 a value. For example, Grade is defined by a function fGrade that maps each person\nin \u2126 to his or her grade (say, one of A, B, or C). The event Grade = A is a shorthand for\nthe event {\u03c9 \u2208 \u2126 : fGrade(\u03c9) = A}. In our example, we might also have a random variable\nIntelligence that (for simplicity) takes as values either \u201chigh\u201d or \u201clow.\u201d In this case, the event\n\u201cIntelligence = high\u201d refers, as can be expected, to the set of smart (high intelligence) students.\nRandom variables can take di\u001berent sets of values. We can think of categorical (or discrete)\nrandom variables that take one of a few values, as in our two preceding examples. We can also\ntalk about random variables that can take infinitely many values (for example, integer or real\nvalues), such as Height that denotes a student\u2019s height. We use Val(X) to denote the set of\nvalues that a random variable X can take.\nIn most of the discussion in this book we examine either categorical random variables or\nrandom variables that take real values. We will usually use uppercase roman letters X, Y, Z\nto denote random variables. In discussing generic random variables, we often use a lowercase\nletter to refer to a value of a random variable. Thus, we use x to refer to a generic value of X.\nFor example, in statements such as \u201cP (X = x) \u2265 0 for all x \u2208 Val(X).\u201d When we discuss\ncategorical random variables, we use the notation x1, . . . , xk, for k = |Val(X)| (the number\nof elements in Val(X)), when we need to enumerate the specific values of X, for example, in\nstatements such as\nkX i\n=1\nP (X = xi) = 1.\nmultinomial The distribution over such a variable is called a multinomial. In the case of a binary-valued\ndistribution random variable X, where Val(X) = {false, true}, we often use x1 to denote the value true for\nX, and x0 to denote the value false. The distribution of such a random variable is called a\nBernoulli Bernoulli distribution.\ndistribution We also use boldface type to denote sets of random variables. Thus, X, Y , or Z are typically\nused to denote a set of random variables, while x, y, z denote assignments of values to the\n\n\nvariables in these sets. We extend the definition of Val(X) to refer to sets of variables in the\nobvious way. Thus, x is always a member of Val(X). For Y \u2286 X, we use xhY i to refer to the\nassignment within x to the variables in Y . For two assignments x (to X) and y (to Y ), we say\nthat x \u223c y if they agree on the variables in their intersection, that is, xhX \u2229 Y i = yhX \u2229 Y i.\nIn many cases, the notation P(X = x) is redundant, since the fact that x is a value of X\nis already reported by our choice of letter. Thus, in many texts on probability, the identity of a\nrandom variable is not explicitly mentioned, but can be inferred through the notation used for\nits value. Thus, we use P(x) as a shorthand for P(X = x) when the identity of the random\nvariable is clear from the context. Another shorthand notation is that Px refers to a sum\nover all possible values that X can take. Thus, the preceding statement will often appear as\nPx P(x) = 1. Finally, another standard notation has to do with conjunction. Rather than write\nP((X = x) \u2229 (Y = y)), we write P(X = x, Y = y), or just P(x, y).\n\n\n3.3 Marginal and Joint Distributions\n\n\nOnce we define a random variable X, we can consider the distribution over events that can be\nmarginal described using X. This distribution is often referred to as the marginal distribution over the\ndistribution random variable X. We denote this distribution by P(X).\nReturning to our population example, consider the random variable Intelligence. The marginal\ndistribution over Intelligence assigns probability to specific events such as P(Intelligence = high)\nand P(Intelligence = low), as well as to the trivial event P(Intelligence \u2208 {high, low}). Note\nthat these probabilities are defined by the probability distribution over the original space. For\nconcreteness, suppose that P(Intelligence = high) = 0.3, P(Intelligence = low) = 0.7.\nIf we consider the random variable Grade, we can also define a marginal distribution. This is a\ndistribution over all events that can be described in terms of the Grade variable. In our example,\nwe have that P(Grade = A) = 0.25, P(Grade = B) = 0.37, and P(Grade = C) = 0.38.\nIt should be fairly obvious that the marginal distribution is a probability distribution satisfying\nthe properties of definition 2.1. In fact, the only change is that we restrict our attention to the\nsubsets of S that can be described with the random variable X.\nIn many situations, we are interested in questions that involve the values of several random\nvariables. For example, we might be interested in the event \u201cIntelligence = high and Grade = A.\u201d\njoint distribution To discuss such events, we need to consider the joint distribution over these two random\nvariables. In general, the joint distribution over a set X = {X1, . . . , Xn} of random variables\nis denoted by P(X1, . . . , Xn) and is the distribution that assigns probabilities to events that\nare specified in terms of these random variables. We use \u03be to refer to a full assignment to the\nvariables in X , that is, \u03be \u2208 Val(X).\nThe joint distribution of two random variables has to be consistent with the marginal distribution, in that P(x) = P\ny P(x, y). This relationship is shown in figure 2.1, where we compute\nthe marginal distribution over Grade by summing the probabilities along each row. Similarly,\nwe find the marginal distribution over Intelligence by summing out along each column. The\nresulting sums are typically written in the row or column margins, whence the term \u201cmarginal\ndistribution.\u201d\nSuppose we have a joint distribution over the variables X = {X1, . . . , Xn}. The most\nfine-grained events we can discuss using these variables are ones of the form \u201cX1 = x1 and\nX2 = x2, . . ., and Xn = xn\u201d for a choice of values x1, . . . , xn for all the variables. Moreover,\n\n\nIntelligence\nlow high\nA 0.07 0.18 0.25\nGrade B 0.28 0.09 0.37\nC 0.35 0.03 0.38\n0.7 0.3 1\nFigure 2.1 Example of a joint distribution P(Intelligence, Grade): Values of Intelligence (columns) and\nGrade (rows) with the associated marginal distribution on each variable.\nany two such events must be either identical or disjoint, since they both assign values to all the\nvariables in X . In addition, any event defined using variables in X must be a union of a set of\ncanonical such events. Thus, we are e\u001bectively working in a canonical outcome space: a space where each\noutcome space outcome corresponds to a joint assignment to X1, . . . , Xn. More precisely, all our probability\ncomputations remain the same whether we consider the original outcome space (for example,\nall students), or the canonical space (for example, all combinations of intelligence and grade).\natomic outcome We use \u03be to denote these atomic outcomes: those assigning a value to each variable in X . For\nexample, if we let X = {Intelligence, Grade}, there are six atomic outcomes, shown in figure 2.1.\nThe figure also shows one possible joint distribution over these six outcomes.\nBased on this discussion, from now on we will not explicitly specify the set of outcomes and\nmeasurable events, and instead implicitly assume the canonical outcome space.\n\n\n3.4 Conditional Probability\n\n\nThe notion of conditional probability extends to induced distributions over random variables. For\nconditional example, we use the notation P (Intelligence | Grade = A) to denote the conditional distribution\ndistribution over the events describable by Intelligence given the knowledge that the student\u2019s grade is A.\nNote that the conditional distribution over a random variable given an observation of the value\nof another one is not the same as the marginal distribution. In our example, P (Intelligence =\nhigh) = 0.3, and P (Intelligence = high | Grade = A) = 0.18/0.25 = 0.72. Thus, clearly\nP (Intelligence | Grade = A) is di\u001berent from the marginal distribution P (Intelligence). The latter\ndistribution represents our prior knowledge about students before learning anything else about a\nparticular student, while the conditional distribution represents our more informed distribution\nafter learning her grade.\nWe will often use the notation P (X | Y ) to represent a set of conditional probability\ndistributions. Intuitively, for each value of Y , this object assigns a probability over values of X\nusing the conditional probability. This notation allows us to write the shorthand version of the\nchain rule: P (X, Y ) = P (X)P (Y | X), which can be extended to multiple variables as\nP (X1, . . . , Xk) = P (X1)P (X2 | X1) \u00b7 \u00b7 \u00b7 P (Xk | X1, . . . , Xk\u22121). (2.5)\nSimilarly, we can state Bayes\u2019 rule in terms of conditional probability distributions:\nP (X | Y ) = P (X)P (Y | X)\nP (Y ) . (2.6)\n\n\n4 Independence and Conditional Independence\n\n\n4.1 Independence\n\n\nAs we mentioned, we usually expect \n P(\u03b1 | \u03b2) \n to be different from \n P(\u03b1) \n. That is, learning that \n \u03b2 \n is true changes our probability over \n \u03b1 \n. However, in some situations equality can occur, so that \n P(\u03b1 | \u03b2) = P(\u03b1) \n. That is, learning that \n \u03b2 \n occurs did not change our probability of \n \u03b1 \n.\n\n\nDefinition\n \nindependent events\n\n\nWe say that an event \n \u03b1 \n is\n independent \nof event \n \u03b2 \n in \n P \n, \ndenoted\n \n P \\models (\u03b1 \u22a5 \u03b2) \n, if \n P(\u03b1 | \u03b2) = P(\u03b1) \n\n\nor if\n \n P(\u03b2) = 0 \n.\n\n\nWe can also provide an alternative definition for the concept of independence:\n\n\nProposition 2.1\n\n\nA distribution \n P \n satisfies \n (\u03b1 \u22a5 \u03b2) \n if and only if \n P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2) \n.\n\n\nPROOF Consider first the case where \n P(\u03b2) = 0 \n; here, we also have \n P(\u03b1 \u2229 \u03b2) = 0 \n, and so the equivalence immediately holds. When \n P(\u03b2) \\neq 0 \n, we can use the chain rule; we write \n P(\u03b1\u2229\u03b2) = P(\u03b1 | \u03b2)P(\u03b2) \n. Since \n \u03b1 \n is independent of \n \u03b2 \n, we have that \n P(\u03b1 | \u03b2) = P(\u03b1) \n. Thus, \n P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2) \n. Conversely, suppose that \n P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2) \n. Then, by definition, we have that\n\n\n\n\n P(\u03b1 | \u03b2) = \\frac{P(\u03b1 \u2229 \u03b2)}{P(\u03b2)} = \\frac{P(\u03b1)P(\u03b2)}{P(\u03b2)} = P(\u03b1). \n\n\n\n\nAs an immediate consequence of this alternative definition, we see that independence is a\nsymmetric notion. That is, (\u03b1 \u22a5 \u03b2) implies (\u03b2 \u22a5 \u03b1).\nExample 2.3 For example, suppose that we toss two coins, and let \u03b1 be the event \u201cthe first toss results in a head\u201d\nand \u03b2 the event \u201cthe second toss results in a head.\u201d It is not hard to convince ourselves that we\nexpect that these two events to be independent. Learning that \u03b2 is true would not change our\nprobability of \u03b1. In this case, we see two di\u001berent physical processes (that is, coin tosses) leading\nto the events, which makes it intuitive that the probabilities of the two are independent. In certain\ncases, the same process can lead to independent events. For example, consider the event \u03b1 denoting\n\u201cthe die outcome is even\u201d and the event \u03b2 denoting \u201cthe die outcome is 1 or 2.\u201d It is easy to check\nthat if the die is fair (each of the six possible outcomes has probability 1 6 ), then these two events are\nindependent.\n\n\n4.2 Conditional Independence\n\n\n\u0011 While independence is a useful property, it is not often that we encounter two indepen- dent events. A more common situation is when two events are independent given an\nadditional event. For example, suppose we want to reason about the chance that our student\nis accepted to graduate studies at Stanford or MIT. Denote by Stanford the event \u201cadmitted to\nStanford\u201d and by MIT the event \u201cadmitted to MIT.\u201d In most reasonable distributions, these two\nevents are not independent. If we learn that a student was admitted to Stanford, then our\nestimate of her probability of being accepted at MIT is now higher, since it is a sign that she is\na promising student\n\n\nNow, suppose that both universities base their decisions only on the student\u2019s grade point\naverage (GPA), and we know that our student has a GPA of A. In this case, we might argue\nthat learning that the student was admitted to Stanford should not change the probability that\nshe will be admitted to MIT: Her GPA already tells us the information relevant to her chances\nof admission to MIT, and finding out about her admission to Stanford does not change that.\nFormally, the statement is\nP(MIT | Stanford, GradeA) = P(MIT | GradeA).\nIn this case, we say that MIT is conditionally independent of Stanford given GradeA.\nDefinition 2.3 We say that an event \u03b1 is conditionally independent of event \u03b2 given event \u03b3 in P, denoted\nconditional\nindependence\nP |= (\u03b1 \u22a5 \u03b2 | \u03b3), if P(\u03b1 | \u03b2 \u2229 \u03b3) = P(\u03b1 | \u03b3) or if P(\u03b2 \u2229 \u03b3) = 0.\nIt is easy to extend the arguments we have seen in the case of (unconditional) independencies\nto give an alternative definition.\nProposition 2.2 P satisfies (\u03b1 \u22a5 \u03b2 | \u03b3) if and only if P(\u03b1 \u2229 \u03b2 | \u03b3) = P(\u03b1 | \u03b3)P(\u03b2 | \u03b3).\n\n\n4.3 Independence of Random Variables\n\n\nUntil now, we have focused on independence between events. Thus, we can say that two events,\nsuch as one toss landing heads and a second also landing heads, are independent. However, we\nwould like to say that any pair of outcomes of the coin tosses is independent. To capture such\nstatements, we can examine the generalization of independence to sets of random variables.\nDefinition 2.4 Let X, Y , Z be sets of random variables. We say that X is conditionally independent of Y given\nconditional\nindependence\nZ in a distribution P if P satisfies (X = x \u22a5 Y = y | Z = z) for all values x \u2208 Val(X),\ny \u2208 Val(Y ), and z \u2208 Val(Z). The variables in the set Z are often said to be observed. If the set\nobserved variable Z is empty, then instead of writing (X \u22a5 Y | \u2205), we write (X \u22a5 Y ) and say that X and Y\nare marginally independent.\nmarginal\nindependence\nThus, an independence statement over random variables is a universal quantification over all\npossible values of the random variables.\nThe alternative characterization of conditional independence follows immediately:\nProposition 2.3 The distribution P satisfies (X \u22a5 Y | Z) if and only if P(X, Y | Z) = P(X | Z)P(Y | Z).\nSuppose we learn about a conditional independence. Can we conclude other independence\nproperties that must hold in the distribution? We have already seen one such example:\nsymmetry \u2022 Symmetry:\n(X \u22a5 Y | Z) =\u21d2 (Y \u22a5 X | Z). (2.7)\nThere are several other properties that hold for conditional independence, and that often\nprovide a very clean method for proving important properties about distributions. Some key\nproperties are:\n\n\n\u2022 Decomposition:\n(X \u22a5 Y , W | Z) =\u21d2 (X \u22a5 Y | Z). (2.8)\nweak union \u2022 Weak union:\n(X \u22a5 Y , W | Z) =\u21d2 (X \u22a5 Y | Z, W). (2.9)\ncontraction \u2022 Contraction:\n(X \u22a5 W | Z, Y )\n(X \u22a5 Y | Z) =\u21d2 (X \u22a5 Y , W | Z). (2.10)\nAn additional important property does not hold in general, but it does hold in an important\nsubclass of distributions.\nDefinition 2.5 A distribution P is said to be positive if for all events \u03b1 \u2208 S such that \u03b1 6= \u2205, we have that\npositive\ndistribution\nP(\u03b1) \n 0.\nFor positive distributions, we also have the following property:\nintersection \u2022 Intersection: For positive distributions, and for mutually disjoint sets X, Y , Z, W :\n(X \u22a5 Y | Z, W)\n(X \u22a5 W | Z, Y ) =\u21d2 (X \u22a5 Y , W | Z). (2.11)\nThe proof of these properties is not di\u001ecult. For example, to prove Decomposition, assume\nthat (X \u22a5 Y, W | Z) holds. Then, from the definition of conditional independence, we have\nthat P(X, Y, W | Z) = P(X | Z)P(Y, W | Z). Now, using basic rules of probability and\narithmetic, we can show\nP(X, Y | Z) = X\nw\nP(X, Y, w | Z)\n= X\nw\nP(X | Z)P(Y, w | Z)\n= P(X | Z) X\nw\nP(Y, w | Z)\n= P(X | Z)P(Y | Z).\nThe only property we used here is called \u201creasoning by cases\u201d (see exercise 2.6). We conclude\nthat (X \u22a5 Y | Z).\n\n\n5 Querying a Distribution\n\n\nOur focus throughout this book is on using a joint probability distribution over multiple random\nvariables to answer queries of interest.\n\n\n5.1 Probability Queries\n\n\nprobability query Perhaps the most common query type is the probability query. Such a query consists of two\nparts:\nevidence \u2022 The evidence: a subset E of random variables in the model, and an instantiation e to these\nvariables;\nquery variables \u2022 the query variables: a subset Y of random variables in the network.\nOur task is to compute\nP(Y | E = e),\nposterior that is, the posterior probability distribution over the values y of Y , conditioned on the fact that\ndistribution E = e. This expression can also be viewed as the marginal over Y , in the distribution we\nobtain by conditioning on e.\n\n\n5.2 MAP Queries\n\n\nA second important type of task is that of finding a high-probability joint assignment to some\nsubset of variables. The simplest variant of this type of task is the MAP query (also called\nMAP assignment most probable explanation (MPE)), whose aim is to find the MAP assignment \u2014 the most likely\nassignment to all of the (non-evidence) variables. More precisely, if we let W = X \u2212 E, our\ntask is to find the most likely assignment to the variables in W given the evidence E = e:\nMAP(W | e) = argmax\nw\nP(w, e), (2.12)\nwhere, in general, argmaxx f(x) represents the value of x for which f(x) is maximal. Note\nthat there might be more than one assignment that has the highest posterior probability. In this\ncase, we can either decide that the MAP task is to return the set of possible assignments, or to\nreturn an arbitrary member of that set.\nIt is important to understand the di\u001berence between MAP queries and probability queries. In\na MAP query, we are finding the most likely joint assignment to W . To find the most likely\nassignment to a single variable A, we could simply compute P(A | e) and then pick the most\nlikely value. However, the assignment where each variable individually picks its most\n\u0011 likely value can be quite di\u001berent from the most likely joint assignment to all variables\nsimultaneously. This phenomenon can occur even in the simplest case, where we have no\nevidence.\nExample 2.4 Consider a two node chain A \u2192 B where A and B are both binary-valued. Assume that:\na0 a1\n0.4 0.6\nA b0 b1\na0 0.1 0.9\na1 0.5 0.5\n(2.13)\nWe can see that P(a1) \n P(a0), so that MAP(A) = a1. However, MAP(A, B) = (a0, b1): Both\nvalues of B have the same probability given a1. Thus, the most likely assignment containing a1 has\nprobability 0.6 \u00d7 0.5 = 0.3. On the other hand, the distribution over values of B is more skewed\ngiven a0, and the most likely assignment (a0, b1) has the probability 0.4 \u00d7 0.9 = 0.36. Thus, we\nhave that argmaxa,b P(a, b) 6= (argmaxa P(a),argmaxb P(b)).\n\n\n5.3 Marginal MAP Queries\n\n\nTo motivate our second query type, let us return to the phenomenon demonstrated in example 2.4. Now, consider a medical diagnosis problem, where the most likely disease has multiple\npossible symptoms, each of which occurs with some probability, but not an overwhelming probability. On the other hand, a somewhat rarer disease might have only a few symptoms, each\nof which is very likely given the disease. As in our simple example, the MAP assignment to\nthe data and the symptoms might be higher for the second disease than for the first one. The\nsolution here is to look for the most likely assignment to the disease variable(s) only, rather than\nthe most likely assignment to both the disease and symptom variables. This approach suggests\nmarginal MAP the use of a more general query type. In the marginal MAP query, we have a subset of variables\nY that forms our query. The task is to find the most likely assignment to the variables in Y\ngiven the evidence E = e:\nMAP(Y | e) = arg max\ny\nP(y | e).\nIf we let Z = X \u2212 Y \u2212 E, the marginal MAP task is to compute:\nMAP(Y | e) = arg max\nY\nX Z\nP(Y , Z | e).\nThus, marginal MAP queries contain both summations and maximizations; in a way, it contains\nelements of both a conditional probability query and a MAP query.\nNote that example 2.4 shows that marginal MAP assignments are not monotonic: the most\nlikely assignment MAP(Y1 | e) might be completely di\u001berent from the assignment to Y1 in\nMAP({Y1, Y2} | e). Thus, in particular, we cannot use a MAP query to give us the correct\nanswer to a marginal MAP query.\n\n\n6 Continuous Spaces\n\n\nIn the previous section, we focused on random variables that have a finite set of possible values.\nIn many situations, we also want to reason about continuous quantities such as weight, height,\nduration, or cost that take real numbers in IR.\nWhen dealing with probabilities over continuous random variables, we have to deal with some\ntechnical issues. For example, suppose that we want to reason about a random variable X that\ncan take values in the range between 0 and 1. That is, Val(X) is the interval [0, 1]. Moreover,\nassume that we want to assign each number in this range equal probability. What would be the\nprobability of a number x? Clearly, since each x has the same probability, and there are infinite\nnumber of values, we must have that P(X = x) = 0. This problem appears even if we do not\nrequire uniform probability.\n\n\n6.1 Probability Density Functions\n\n\nHow do we define probability over a continuous random variable? We say that a function\ndensity function p : IR 7\u2192 IR is a probability density function or (PDF) for X if it is a nonnegative integrable\n\n\nfunction such that\nZ\nVal(X)\np(x)dx = 1.\nThat is, the integral over the set of possible values of X is 1. The PDF defines a distribution for\nX as follows: for any x in our event space:\nP(X \u2264 a) =\naZ\n\u2212\u221e\np(x)dx.\ncumulative The function P is the cumulative distribution for X. We can easily employ the rules of\ndistribution probability to see that by using the density function we can evaluate the probability of other\nevents. For example,\nP(a \u2264 X \u2264 b) =\nbZa\np(x)dx.\nIntuitively, the value of a PDF p(x) at a point x is the incremental amount that x adds to the\ncumulative distribution in the integration process. The higher the value of p at and around x,\nthe more mass is added to the cumulative distribution as it passes x.\nThe simplest PDF is the uniform distribution.\nDefinition 2.6 A variable X has a uniform distribution over [a, b], denoted X \u223c Unif[a,b] if it has the PDF\nuniform\ndistribution\np(x) = \u001a 0 otherwise b\u2212 1a b \u2265 x \u2265 a.\nThus, the probability of any subinterval of [a, b] is proportional its size relative to the size of\n[a, b]. Note that, if b \u2212 a \n 1, then the density can be greater than 1. Although this looks\nunintuitive, this situation can occur even in a legal PDF, if the interval over which the value is\ngreater than 1 is not too large. We have only to satisfy the constraint that the total area under\nthe PDF is 1.\nAs a more complex example, consider the Gaussian distribution.\nDefinition 2.7 A random variable X has a Gaussian distribution with mean \u00b5 and variance \u03c32, denoted X \u223c\nGaussian\ndistribution\nN \u00b5; \u03c32\u0001, if it has the PDF\np(x) = \u221a21\u03c0\u03c3 e\u2212 (x2 \u2212 \u03c3\u00b5 2)2 .\nstandard A standard Gaussian is one with mean 0 and variance 1.\nGaussian\nA Gaussian distribution has a bell-like curve, where the mean parameter \u00b5 controls the\nlocation of the peak, that is, the value for which the Gaussian gets its maximum value. The\nvariance parameter \u03c32 determines how peaked the Gaussian is: the smaller the variance, the\n\n\n\n\n\n\n\n\nmore peaked the Gaussian. Figure 2.2 shows the probability density function of a few di\u001berent\nGaussian distributions.\nMore technically, the probability density function is specified as an exponential, where the\nexpression in the exponent corresponds to the square of the number of standard deviations \u03c3\nthat x is away from the mean \u00b5. The probability of x decreases exponentially with the square\nof its deviation from the mean, as measured in units of its standard deviation.\n\n\n6.2 Joint Density Functions\n\n\nThe discussion of density functions for a single variable naturally extends for joint distributions\nof continuous random variables.\nDefinition 2.8 Let P be a joint distribution over continuous random variables X1, . . . , Xn. A function p(x1, . . . , xn)\njoint density is a joint density function of X1, . . . , Xn if\n\u2022 p(x1, . . . , xn) \u2265 0 for all values x1, . . . , xn of X1, . . . , Xn.\n\u2022 p is an integrable function.\n\u2022 For any choice of a1, . . . , an, and b1, . . . , bn,\nP(a1 \u2264 X1 \u2264 b1, . . . , an \u2264 Xn \u2264 bn) =\nb1\nZa1\n\u00b7 \u00b7 \u00b7\nb\nnZ\na\nn\np(x1, . . . , xn)dx1 . . . dxn.\nThus, a joint density specifies the probability of any joint event over the variables of interest.\nBoth the uniform distribution and the Gaussian distribution have natural extensions to the\nmultivariate case. The definition of a multivariate uniform distribution is straightforward. We\ndefer the definition of the multivariate Gaussian to section 7.1.\nFrom the joint density we can derive the marginal density of any random variable by integrating out the other variables. Thus, for example, if p(x, y) is the joint density of X and Y\n\n\nthen\np(x) =\n\u221e Z\n\u2212\u221e\np(x, y)dy.\nTo see why this equality holds, note that the event a \u2264 X \u2264 b is, by definition, equal to the\nevent \u201ca \u2264 X \u2264 b and \u2212\u221e \u2264 Y \u2264 \u221e.\u201d This rule is the direct analogue of marginalization for\ndiscrete variables. Note that, as with discrete probability distributions, we abuse notation a bit\nand use p to denote both the joint density of X and Y and the marginal density of X. In cases\nwhere the distinction is not clear, we use subscripts, so that pX will be the marginal density, of\nX, and pX,Y the joint density.\n\n\n6.3 Conditional Density Functions\n\n\nAs with discrete random variables, we want to be able to describe conditional distributions of\ncontinuous variables. Suppose, for example, we want to define P(Y | X = x). Applying the\ndefinition of conditional distribution (equation (2.1)), we run into a problem, since P(X = x) =\n0. Thus, the ratio of P(Y, X = x) and P(X = x) is undefined.\nTo avoid this problem, we might consider conditioning on the event x \u2212 \u000f \u2264 X \u2264 x + \u000f,\nwhich can have a positive probability. Now, the conditional probability is well defined. Thus, we\nmight consider the limit of this quantity when \u000f \u2192 0. We define\nP(Y | x) = lim\n\u000f\u21920\nP(Y | x \u2212 \u000f \u2264 X \u2264 x + \u000f).\nWhen does this limit exist? If there is a continuous joint density function p(x, y), then we can\nderive the form for this term. To do so, consider some event on Y , say a \u2264 Y \u2264 b. Recall that\nP(a \u2264 Y \u2264 b | x \u2212 \u000f \u2264 X \u2264 x + \u000f) = P(a \u2264 Y \u2264 b, x \u2212 \u000f \u2264 X \u2264 x + \u000f)\nP(x \u2212 \u000f \u2264 X \u2264 x + \u000f)\n=\nRa b Rx x\u2212 +\u000f\u000f p(x0, y)dydx0\nRx x\u2212 +\u000f\u000f p(x0)dx0 .\nWhen \u000f is su\u001eciently small, we can approximate\nx+\u000f\nZ\nx\u2212\u000f\np(x0)dx0 \u2248 2\u000fp(x).\nUsing a similar approximation for p(x0, y), we get\nP(a \u2264 Y \u2264 b | x \u2212 \u000f \u2264 X \u2264 x + \u000f) \u2248\nRa b 2\u000fp(x, y)dy\n2\u000fp(x)\n=\nbZa\np(x, y)\np(x) dy.\nWe conclude that p(x,y)\np(x) is the density of P(Y | X = x).\n\n\nLet p(x, y) be the joint density of X and Y . The conditional density function of Y given X is\nconditional\ndensity function\ndefined as\np(y | x) = p(x, y)\np(x)\nWhen p(x) = 0, the conditional density is undefined.\nThe conditional density p(y | x) characterizes the conditional distribution P(Y | X = x) we\ndefined earlier.\nThe properties of joint distributions and conditional distributions carry over to joint and\nconditional density functions. In particular, we have the chain rule\np(x, y) = p(x)p(y | x) (2.14)\nand Bayes\u2019 rule\np(x | y) = p(x)p(y | x)\np(y) . (2.15)\nAs a general statement, whenever we discuss joint distributions of continuous random variables, we discuss properties with respect to the joint density function instead of the joint\ndistribution, as we do in the case of discrete variables. Of particular interest is the notion of\n(conditional) independence of continuous random variables.\nDefinition 2.10 Let X, Y , and Z be sets of continuous random variables with joint density p(X, Y , Z). We say\nconditional that X is conditionally independent of Y given Z if\nindependence\np(x | z) = p(x | y, z) for all x, y, z such that p(z) \n 0.\n\n\n7 Expectation and Variance\n\n\n7.1 Expectation\n\n\nexpectation Let X be a discrete random variable that takes numerical values; then the expectation of X\nunder the distribution P is\nIEP[X] = X\nx\nx \u00b7 P(x).\nIf X is a continuous variable, then we use the density function\nIEP[X] = Z x \u00b7 p(x)dx.\nFor example, if we consider X to be the outcome of rolling a fair die with probability 1/6\nfor each outcome, then IE[X] = 1 \u00b7 1 6 + 2 \u00b7 1 6 + \u00b7 \u00b7 \u00b7 + 6 \u00b7 1 6 = 3.5. On the other hand, if\nwe consider a biased die where P(X = 6) = 0.5 and P(X = x) = 0.1 for x \n 6, then\nIE[X] = 1 \u00b7 0.1 + \u00b7 \u00b7 \u00b7 + 5 \u00b7 0.1 + \u00b7 \u00b7 \u00b7 + 6 \u00b7 0.5 = 4.5.\n\n\nOften we are interested in expectations of a function of a random variable (or several random\nvariables). Thus, we might consider extending the definition to consider the expectation of a\nfunctional term such as X2 + 0.5X. Note, however, that any function g of a set of random\nvariables X1, . . . , Xk is essentially defining a new random variable Y : For any outcome \u03c9 \u2208 \u2126,\nwe define the value of Y as g(fX1(\u03c9), . . . , fXk(\u03c9)).\nBased on this discussion, we often define new random variables by a functional term. For\nexample Y = X2, or Y = eX. We can also consider functions that map values of one or more\ncategorical random variables to numerical values. One such function that we use quite often is\nindicator function the indicator function, which we denote 11{X = x}. This function takes value 1 when X = x,\nand 0 otherwise.\nIn addition, we often consider expectations of functions of random variables without bothering\nto name the random variables they define. For example IEP [X + Y ]. Nonetheless, we should\nkeep in mind that such a term does refer to an expectation of a random variable.\nWe now turn to examine properties of the expectation of a random variable.\nFirst, as can be easily seen, the expectation of a random variable is a linear function in that\nrandom variable. Thus,\nIE[a \u00b7 X + b] = aIE[X] + b.\nA more complex situation is when we consider the expectation of a function of several random\nvariables that have some joint behavior. An important property of expectation is that the\nexpectation of a sum of two random variables is the sum of the expectations.\nProposition 2.4 IE[X + Y ] = IE[X] + IE[Y ].\nlinearity of This property is called linearity of expectation. It is important to stress that this identity is true\nexpectation even when the variables are not independent. As we will see, this property is key in simplifying\nmany seemingly complex problems.\nFinally, what can we say about the expectation of a product of two random variables? In\ngeneral, very little:\nExample 2.5 Consider two random variables X and Y , each of which takes the value +1 with probability 1/2,\nand the value \u22121 with probability 1/2. If X and Y are independent, then IE[X \u00b7 Y ] = 0. On the\nother hand, if X and Y are correlated in that they always take the same value, then IE[X \u00b7 Y ] = 1.\nHowever, when X and Y are independent, then, as in our example, we can compute the\nexpectation simply as a product of their individual expectations:\nProposition 2.5 If X and Y are independent, then\nIE[X \u00b7 Y ] = IE[X] \u00b7 IE[Y ].\nconditional We often also use the expectation given some evidence. The conditional expectation of X\nexpectation given y is\nIEP [X | y] = X\nx\nx \u00b7 P(x | y).\n\n\n7.2 Variance\n\n\nThe expectation of X tells us the mean value of X. However, It does not indicate how far X\nvariance deviates from this value. A measure of this deviation is the variance of X.\nVVarP [X] = IEP h(X \u2212 IEP [X])2i.\nThus, the variance is the expectation of the squared di\u001berence between X and its expected\nvalue. It gives us an indication of the spread of values of X around the expected value.\nAn alternative formulation of the variance is\nVVar[X] = IEX2 \u2212 (IE[X])2 . (2.16)\n(see exercise 2.11).\nSimilar to the expectation, we can consider the expectation of a functions of random variables.\nProposition 2.6 If X and Y are independent, then\nVVar[X + Y ] = VVar[X] + VVar[Y ].\nIt is straightforward to show that the variance scales as a quadratic function of X. In\nparticular, we have:\nVVar[a \u00b7 X + b] = a2VVar[X].\nFor this reason, we are often interested in the square root of the variance, which is called the\nstandard standard deviation of the random variable. We define\ndeviation\n\u03c3X = pVVar[X].\nThe intuition is that it is improbable to encounter values of X that are farther than several\nstandard deviations from the expected value of X. Thus, \u03c3X is a normalized measure of\n\u201cdistance\u201d from the expected value of X.\nAs an example consider the Gaussian distribution of definition 2.7.\nProposition 2.7 Let X be a random variable with Gaussian distribution N(\u00b5, \u03c32), then IE[X] = \u00b5 and VVar[X] =\n\u03c32.\nThus, the parameters of the Gaussian distribution specify the expectation and the variance of\nthe distribution. As we can see from the form of the distribution, the density of values of X\ndrops exponentially fast in the distance x\u2212\u00b5\n\u03c3\n.\nNot all distributions show such a rapid decline in the probability of outcomes that are distant\nfrom the expectation. However, even for arbitrary distributions, one can show that there is a\ndecline.\nTheorem 2.1 (Chebyshev inequality):\nChebyshev\u2019s\ninequality\nP (|X \u2212 IEP [X]| \u2265 t) \u2264 VVarP [X]\nt2 .\n\n\nWe can restate this inequality in terms of standard deviations: We write t = k\u03c3X to get\nP(|X \u2212 IEP [X]| \u2265 k\u03c3X) \u2264 1\nk2.\nThus, for example, the probability of X being more than two standard deviations away from\nIE[X] is less than 1/4.", 
            "title": "Foundation: Probability"
        }, 
        {
            "location": "/foundation_probability/#foundation-probability-theory", 
            "text": "The main focus of this book is on complex probability distributions. In this section we briefly\nreview basic concepts from probability theory.", 
            "title": "Foundation: Probability Theory"
        }, 
        {
            "location": "/foundation_probability/#1-probability-distributions", 
            "text": "When we use the word \u201cprobability\u201d in day-to-day life, we refer to a degree of confidence that\nan event of an uncertain nature will occur. For example, the weather report might say \u201cthere\nis a low probability of light rain in the afternoon.\u201d Probability theory deals with the formal\nfoundations for discussing such estimates and the rules they should obey.\nBefore we discuss the representation of probability, we need to define what the events are to\nwhich we want to assign a probability. These events might be di\u001berent outcomes of throwing\na die, the outcome of a horse race, the weather configurations in California, or the possible\nfailures of a piece of machinery.", 
            "title": "1 Probability Distributions"
        }, 
        {
            "location": "/foundation_probability/#11-event-spaces", 
            "text": "event Formally, we define events by assuming that there is an agreed upon space of possible outcomes,\noutcome space which we denote by \u2126. For example, if we consider dice, we might set \u2126 = {1, 2, 3, 4, 5, 6}. In\nthe case of a horse race, the space might be all possible orders of arrivals at the finish line, a\nmuch larger space.  measurable event In addition, we assume that there is a set of measurable events S to which we are willing to\nassign probabilities. Formally, each event \u03b1 \u2208 S is a subset of \u2126. In our die example, the event\n{6} represents the case where the die shows 6, and the event {1, 3, 5} represents the case of\nan odd outcome. In the horse-race example, we might consider the event \u201cLucky Strike wins,\u201d\nwhich contains all the outcomes in which the horse Lucky Strike is first.\nProbability theory requires that the event space satisfy three basic properties:\n\u2022 It contains the empty event \u2205, and the trivial event \u2126.\n\u2022 It is closed under union. That is, if \u03b1, \u03b2 \u2208 S, then so is \u03b1 \u222a \u03b2.\n\u2022 It is closed under complementation. That is, if \u03b1 \u2208 S, then so is \u2126 \u2212 \u03b1.\nThe requirement that the event space is closed under union and complementation implies that\nit is also closed under other Boolean operations, such as intersection and set di\u001berence.", 
            "title": "1.1 Event Spaces"
        }, 
        {
            "location": "/foundation_probability/#12-probability-distributions", 
            "text": "Definition 2.1 A probability distribution P over (\u2126, S) is a mapping from events in S to real values that satisfies\nprobability\ndistribution\nthe following conditions:\n\u2022 P(\u03b1) \u2265 0 for all \u03b1 \u2208 S.\n\u2022 P(\u2126) = 1.\n\u2022 If \u03b1, \u03b2 \u2208 S and \u03b1 \u2229 \u03b2 = \u2205, then P(\u03b1 \u222a \u03b2) = P(\u03b1) + P(\u03b2).\nThe first condition states that probabilities are not negative. The second states that the \u201ctrivial\nevent,\u201d which allows all possible outcomes, has the maximal possible probability of 1. The third\ncondition states that the probability that one of two mutually disjoint events will occur is the\nsum of the probabilities of each event. These two conditions imply many other conditions. Of\nparticular interest are P(\u2205) = 0, and P(\u03b1 \u222a \u03b2) = P(\u03b1) + P(\u03b2) \u2212 P(\u03b1 \u2229 \u03b2).", 
            "title": "1.2 Probability Distributions"
        }, 
        {
            "location": "/foundation_probability/#13-interpretations-of-probability", 
            "text": "Before we continue to discuss probability distributions, we need to consider the interpretations\nthat we might assign to them. Intuitively, the probability P(\u03b1) of an event \u03b1 quantifies the\ndegree of confidence that \u03b1 will occur. If P(\u03b1) = 1, we are certain that one of the outcomes\nin \u03b1 occurs, and if P(\u03b1) = 0, we consider all of them impossible. Other probability values\nrepresent options that lie between these two extremes.\nThis description, however, does not provide an answer to what the numbers mean. There are\ntwo common interpretations for probabilities.\nfrequentist The frequentist interpretation views probabilities as frequencies of events. More precisely, the\ninterpretation probability of an event is the fraction of times the event occurs if we repeat the experiment\nindefinitely. For example, suppose we consider the outcome of a particular die roll. In this case,\nthe statement P(\u03b1) = 0.3, for \u03b1 = {1, 3, 5}, states that if we repeatedly roll this die and record\nthe outcome, then the fraction of times the outcomes in \u03b1 will occur is 0.3. More precisely, the\nlimit of the sequence of fractions of outcomes in \u03b1 in the first roll, the first two rolls, the first\nthree rolls, . . ., the first n rolls, . . . is 0.3.  The frequentist interpretation gives probabilities a tangible semantics. When we discuss\nconcrete physical systems (for example, dice, coin flips, and card games) we can envision how\nthese frequencies are defined. It is also relatively straightforward to check that frequencies must\nsatisfy the requirements of proper distributions.\nThe frequentist interpretation fails, however, when we consider events such as \u201cIt will rain\ntomorrow afternoon.\u201d Although the time span of \u201cTomorrow afternoon\u201d is somewhat ill defined,\nwe expect it to occur exactly once. It is not clear how we define the frequencies of such events.\nSeveral attempts have been made to define the probability for such an event by finding a\nreference class reference class of similar events for which frequencies are well defined; however, none of them\nhas proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory\ninterpretation for a statement such as \u201cthe probability of rain tomorrow afternoon is 0.3.\u201d\n\u0011 An alternative interpretation views probabilities as subjective degrees of belief. Under\nsubjective\ninterpretation\nthis interpretation, the statement P(\u03b1) = 0.3 represents a subjective statement about\none\u2019s own degree of belief that the event \u03b1 will come about. Thus, the statement \u201cthe\nprobability of rain tomorrow afternoon is 50 percent\u201d tells us that in the opinion of the speaker,\nthe chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon\nwill occur only once, we can still have uncertainty about its outcome, and represent it using\nnumbers (that is, probabilities).\nThis description still does not resolve what exactly it means to hold a particular degree of\nbelief. What stops a person from stating that the probability that Bush will win the election\nis 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need\nto explain how subjective degrees of beliefs (something that is internal to each one of us) are\nreflected in our actions.\nThis issue is a major concern in subjective probabilities. One possible way of attributing\ndegrees of beliefs is by a betting game. Suppose you believe that P(\u03b1) = 0.8. Then you would\nbe willing to place a bet of $1 against $3. To see this, note that with probability 0.8 you gain a\ndollar, and with probability 0.2 you lose $3, so on average this bet is a good deal with expected\ngain of 20 cents. In fact, you might be even tempted to place a bet of $1 against $4. Under\nthis bet the average gain is 0, so you should not mind. However, you would not consider it\nworthwhile to place a bet $1 against $4 and 10 cents, since that would have negative expected\ngain. Thus, by finding which bets you are willing to place, we can assess your degrees of beliefs.\nThe key point of this mental game is the following. If you hold degrees of belief that do not\nsatisfy the rule of probability, then by a clever construction we can find a series of bets that\nwould result in a sure negative outcome for you. Thus, the argument goes, a rational person\nmust hold degrees of belief that satisfy the rules of probability.1\nIn the remainder of the book we discuss probabilities, but we usually do not explicitly state\ntheir interpretation. Since both interpretations lead to the same mathematical rules, the technical\ndefinitions hold for both interpretations.", 
            "title": "1.3 Interpretations of Probability"
        }, 
        {
            "location": "/foundation_probability/#2-basic-concepts-in-probability", 
            "text": "", 
            "title": "2 Basic Concepts in Probability"
        }, 
        {
            "location": "/foundation_probability/#21-conditional-probability", 
            "text": "To use a concrete example, suppose we consider a distribution over a population of students\ntaking a certain course. The space of outcomes is simply the set of all students in the population.\nNow, suppose that we want to reason about the students\u2019 intelligence and their final grade. We\ncan define the event \u03b1 to denote \u201call students with grade A,\u201d and the event \u03b2 to denote \u201call\nstudents with high intelligence.\u201d Using our distribution, we can consider the probability of these\nevents, as well as the probability of \u03b1 \u2229 \u03b2 (the set of intelligent students who got grade A). This,\nhowever, does not directly tell us how to update our beliefs given new evidence. Suppose we\nlearn that a student has received the grade A; what does that tell us about her intelligence?\nThis kind of question arises every time we want to use distributions to reason about the real\nworld. More precisely, after learning that an event \u03b1 is true, how do we change our probability\nconditional about \u03b2 occurring? The answer is via the notion of conditional probability. Formally, the\nprobability conditional probability of \u03b2 given \u03b1 is defined as\nP(\u03b2 | \u03b1) = P(\u03b1 \u2229 \u03b2)\nP(\u03b1) (2.1)\nThat is, the probability that \u03b2 is true given that we know \u03b1 is the relative proportion of outcomes\nsatisfying \u03b2 among these that satisfy \u03b1. (Note that the conditional probability is not defined\nwhen P(\u03b1) = 0.)\nThe conditional probability given an event (say \u03b1) satisfies the properties of definition 2.1 (see\nexercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think\nof the conditioning operation as taking one distribution and returning another over the same\nprobability space.", 
            "title": "2.1 Conditional Probability"
        }, 
        {
            "location": "/foundation_probability/#22-chain-rule-and-bayes-rule", 
            "text": "From the definition of the conditional distribution, we immediately see that\nP(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2 | \u03b1). (2.2)\nchain rule This equality is known as the chain rule of conditional probabilities. More generally, if\n\u03b11, . . . , \u03b1k are events, then we can write\nP(\u03b11 \u2229 . . . \u2229 \u03b1k) = P(\u03b11)P(\u03b12 | \u03b11) \u00b7 \u00b7 \u00b7 P(\u03b1k | \u03b11 \u2229 . . . \u2229 \u03b1k\u22121). (2.3)\nIn other words, we can express the probability of a combination of several events in terms of the\nprobability of the first, the probability of the second given the first, and so on. It is important\nto notice that we can expand this expression using any order of events \u2014 the result will remain\nthe same.\nBayes\u2019 rule Another immediate consequence of the definition of conditional probability is Bayes\u2019 rule\nP(\u03b1 | \u03b2) = P(\u03b2 | \u03b1)P(\u03b1)\nP(\u03b2)  A more general conditional version of Bayes\u2019 rule, where all our probabilities are conditioned on\nsome background event \u03b3, also holds:\nP(\u03b1 | \u03b2 \u2229 \u03b3) = P(\u03b2 | \u03b1 \u2229 \u03b3)P(\u03b1 | \u03b3)\nP(\u03b2 | \u03b3) .\nBayes\u2019 rule is important in that it allows us to compute the conditional probability P(\u03b1 | \u03b2)\nfrom the \u201cinverse\u201d conditional probability P(\u03b2 | \u03b1).\nExample 2.1 Consider the student population, and let Smart denote smart students and GradeA denote students who got grade A. Assume we believe (perhaps based on estimates from past statistics) that\nP(GradeA | Smart) = 0.6, and now we learn that a particular student received grade A. Can\nwe estimate the probability that the student is smart? According to Bayes\u2019 rule, this depends on\nprior our prior probability for students being smart (before we learn anything about them) and the\nprior probability of students receiving high grades. For example, suppose that P(Smart) = 0.3\nand P(GradeA) = 0.2, then we have that P(Smart | GradeA) = 0.6 \u2217 0.3/0.2 = 0.9. That\nis, an A grade strongly suggests that the student is smart. On the other hand, if the test was\neasier and high grades were more common, say, P(GradeA) = 0.4 then we would get that\nP(Smart | GradeA) = 0.6 \u2217 0.3/0.4 = 0.45, which is much less conclusive about the student.\nAnother classic example that shows the importance of this reasoning is in disease screening.\nTo see this, consider the following hypothetical example (none of the mentioned figures are\nrelated to real statistics).\nExample 2.2 Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected,\nthen the test will be positive with probability 0.95, and if the patient is not infected, then the test\nwill be negative with probability 0.95. Now suppose that a person gets a positive test result. What is\nthe probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent\nof the time, then the probability that the subject is infected is 0.95. That is, 95 percent of subjects\nwith positive results have TB.\nIf we consider the problem by applying Bayes\u2019 rule, we see that we need to consider the prior\nprobability of TB infection, and the probability of getting positive test result. Suppose that 1 in\n1000 of the subjects who get tested is infected. That is, P(TB) = 0.001. What is the probability of\ngetting a positive test result? From our description, we see that 0.001 \u00b7 0.95 infected subjects get a\npositive result, and 0.999\u00b70.05 uninfected subjects get a positive result. Thus, P(Positive) = 0.0509.\nApplying Bayes\u2019 rule, we get that P(TB | Positive) = 0.001\u00b70.95/0.0509 \u2248 0.0187. Thus, although\na subject with a positive test is much more probable to be TB-infected than is a random subject,\nfewer than 2 percent of these subjects are TB-infected.", 
            "title": "2.2 Chain Rule and Bayes Rule"
        }, 
        {
            "location": "/foundation_probability/#3-random-variables-and-joint-distributions", 
            "text": "", 
            "title": "3 Random Variables and Joint Distributions"
        }, 
        {
            "location": "/foundation_probability/#31-motivation", 
            "text": "Our discussion of probability distributions deals with events. Formally, we can consider any\nevent from the set of measurable events. The description of events is in terms of sets of\noutcomes. In many cases, however, it would be more natural to consider attributes of the\noutcome. For example, if we consider a patient, we might consider attributes such as \u201cage,\u201d  \u201cgender,\u201d and \u201csmoking history\u201d that are relevant for assigning probability over possible diseases\nand symptoms. We would like then consider events such as \u201cage   55, heavy smoking history,\nand su\u001bers from repeated cough.\u201d\nTo use a concrete example, consider again a distribution over a population of students in a\ncourse. Suppose that we want to reason about the intelligence of students, their final grades,\nand so forth. We can use an event such as GradeA to denote the subset of students that received\nthe grade A and use it in our formulation. However, this discussion becomes rather cumbersome\nif we also want to consider students with grade B, students with grade C, and so on. Instead, we\nwould like to consider a way of directly referring to a student\u2019s grade in a clean, mathematical\nway.\nThe formal machinery for discussing attributes and their values in di\u001berent outcomes are\nrandom variable random variables. A random variable is a way of reporting an attribute of the outcome. For\nexample, suppose we have a random variable Grade that reports the final grade of a student,\nthen the statement P (Grade = A) is another notation for P (GradeA).  n the statement P (Grade = A) is another notation for P (GradeA).", 
            "title": "3.1 Motivation"
        }, 
        {
            "location": "/foundation_probability/#32-what-is-a-random-variable", 
            "text": "Formally, a random variable, such as Grade, is defined by a function that associates with each\noutcome in \u2126 a value. For example, Grade is defined by a function fGrade that maps each person\nin \u2126 to his or her grade (say, one of A, B, or C). The event Grade = A is a shorthand for\nthe event {\u03c9 \u2208 \u2126 : fGrade(\u03c9) = A}. In our example, we might also have a random variable\nIntelligence that (for simplicity) takes as values either \u201chigh\u201d or \u201clow.\u201d In this case, the event\n\u201cIntelligence = high\u201d refers, as can be expected, to the set of smart (high intelligence) students.\nRandom variables can take di\u001berent sets of values. We can think of categorical (or discrete)\nrandom variables that take one of a few values, as in our two preceding examples. We can also\ntalk about random variables that can take infinitely many values (for example, integer or real\nvalues), such as Height that denotes a student\u2019s height. We use Val(X) to denote the set of\nvalues that a random variable X can take.\nIn most of the discussion in this book we examine either categorical random variables or\nrandom variables that take real values. We will usually use uppercase roman letters X, Y, Z\nto denote random variables. In discussing generic random variables, we often use a lowercase\nletter to refer to a value of a random variable. Thus, we use x to refer to a generic value of X.\nFor example, in statements such as \u201cP (X = x) \u2265 0 for all x \u2208 Val(X).\u201d When we discuss\ncategorical random variables, we use the notation x1, . . . , xk, for k = |Val(X)| (the number\nof elements in Val(X)), when we need to enumerate the specific values of X, for example, in\nstatements such as\nkX i\n=1\nP (X = xi) = 1.\nmultinomial The distribution over such a variable is called a multinomial. In the case of a binary-valued\ndistribution random variable X, where Val(X) = {false, true}, we often use x1 to denote the value true for\nX, and x0 to denote the value false. The distribution of such a random variable is called a\nBernoulli Bernoulli distribution.\ndistribution We also use boldface type to denote sets of random variables. Thus, X, Y , or Z are typically\nused to denote a set of random variables, while x, y, z denote assignments of values to the  variables in these sets. We extend the definition of Val(X) to refer to sets of variables in the\nobvious way. Thus, x is always a member of Val(X). For Y \u2286 X, we use xhY i to refer to the\nassignment within x to the variables in Y . For two assignments x (to X) and y (to Y ), we say\nthat x \u223c y if they agree on the variables in their intersection, that is, xhX \u2229 Y i = yhX \u2229 Y i.\nIn many cases, the notation P(X = x) is redundant, since the fact that x is a value of X\nis already reported by our choice of letter. Thus, in many texts on probability, the identity of a\nrandom variable is not explicitly mentioned, but can be inferred through the notation used for\nits value. Thus, we use P(x) as a shorthand for P(X = x) when the identity of the random\nvariable is clear from the context. Another shorthand notation is that Px refers to a sum\nover all possible values that X can take. Thus, the preceding statement will often appear as\nPx P(x) = 1. Finally, another standard notation has to do with conjunction. Rather than write\nP((X = x) \u2229 (Y = y)), we write P(X = x, Y = y), or just P(x, y).", 
            "title": "3.2 What Is a Random Variable?"
        }, 
        {
            "location": "/foundation_probability/#33-marginal-and-joint-distributions", 
            "text": "Once we define a random variable X, we can consider the distribution over events that can be\nmarginal described using X. This distribution is often referred to as the marginal distribution over the\ndistribution random variable X. We denote this distribution by P(X).\nReturning to our population example, consider the random variable Intelligence. The marginal\ndistribution over Intelligence assigns probability to specific events such as P(Intelligence = high)\nand P(Intelligence = low), as well as to the trivial event P(Intelligence \u2208 {high, low}). Note\nthat these probabilities are defined by the probability distribution over the original space. For\nconcreteness, suppose that P(Intelligence = high) = 0.3, P(Intelligence = low) = 0.7.\nIf we consider the random variable Grade, we can also define a marginal distribution. This is a\ndistribution over all events that can be described in terms of the Grade variable. In our example,\nwe have that P(Grade = A) = 0.25, P(Grade = B) = 0.37, and P(Grade = C) = 0.38.\nIt should be fairly obvious that the marginal distribution is a probability distribution satisfying\nthe properties of definition 2.1. In fact, the only change is that we restrict our attention to the\nsubsets of S that can be described with the random variable X.\nIn many situations, we are interested in questions that involve the values of several random\nvariables. For example, we might be interested in the event \u201cIntelligence = high and Grade = A.\u201d\njoint distribution To discuss such events, we need to consider the joint distribution over these two random\nvariables. In general, the joint distribution over a set X = {X1, . . . , Xn} of random variables\nis denoted by P(X1, . . . , Xn) and is the distribution that assigns probabilities to events that\nare specified in terms of these random variables. We use \u03be to refer to a full assignment to the\nvariables in X , that is, \u03be \u2208 Val(X).\nThe joint distribution of two random variables has to be consistent with the marginal distribution, in that P(x) = P\ny P(x, y). This relationship is shown in figure 2.1, where we compute\nthe marginal distribution over Grade by summing the probabilities along each row. Similarly,\nwe find the marginal distribution over Intelligence by summing out along each column. The\nresulting sums are typically written in the row or column margins, whence the term \u201cmarginal\ndistribution.\u201d\nSuppose we have a joint distribution over the variables X = {X1, . . . , Xn}. The most\nfine-grained events we can discuss using these variables are ones of the form \u201cX1 = x1 and\nX2 = x2, . . ., and Xn = xn\u201d for a choice of values x1, . . . , xn for all the variables. Moreover,  Intelligence\nlow high\nA 0.07 0.18 0.25\nGrade B 0.28 0.09 0.37\nC 0.35 0.03 0.38\n0.7 0.3 1\nFigure 2.1 Example of a joint distribution P(Intelligence, Grade): Values of Intelligence (columns) and\nGrade (rows) with the associated marginal distribution on each variable.\nany two such events must be either identical or disjoint, since they both assign values to all the\nvariables in X . In addition, any event defined using variables in X must be a union of a set of\ncanonical such events. Thus, we are e\u001bectively working in a canonical outcome space: a space where each\noutcome space outcome corresponds to a joint assignment to X1, . . . , Xn. More precisely, all our probability\ncomputations remain the same whether we consider the original outcome space (for example,\nall students), or the canonical space (for example, all combinations of intelligence and grade).\natomic outcome We use \u03be to denote these atomic outcomes: those assigning a value to each variable in X . For\nexample, if we let X = {Intelligence, Grade}, there are six atomic outcomes, shown in figure 2.1.\nThe figure also shows one possible joint distribution over these six outcomes.\nBased on this discussion, from now on we will not explicitly specify the set of outcomes and\nmeasurable events, and instead implicitly assume the canonical outcome space.", 
            "title": "3.3 Marginal and Joint Distributions"
        }, 
        {
            "location": "/foundation_probability/#34-conditional-probability", 
            "text": "The notion of conditional probability extends to induced distributions over random variables. For\nconditional example, we use the notation P (Intelligence | Grade = A) to denote the conditional distribution\ndistribution over the events describable by Intelligence given the knowledge that the student\u2019s grade is A.\nNote that the conditional distribution over a random variable given an observation of the value\nof another one is not the same as the marginal distribution. In our example, P (Intelligence =\nhigh) = 0.3, and P (Intelligence = high | Grade = A) = 0.18/0.25 = 0.72. Thus, clearly\nP (Intelligence | Grade = A) is di\u001berent from the marginal distribution P (Intelligence). The latter\ndistribution represents our prior knowledge about students before learning anything else about a\nparticular student, while the conditional distribution represents our more informed distribution\nafter learning her grade.\nWe will often use the notation P (X | Y ) to represent a set of conditional probability\ndistributions. Intuitively, for each value of Y , this object assigns a probability over values of X\nusing the conditional probability. This notation allows us to write the shorthand version of the\nchain rule: P (X, Y ) = P (X)P (Y | X), which can be extended to multiple variables as\nP (X1, . . . , Xk) = P (X1)P (X2 | X1) \u00b7 \u00b7 \u00b7 P (Xk | X1, . . . , Xk\u22121). (2.5)\nSimilarly, we can state Bayes\u2019 rule in terms of conditional probability distributions:\nP (X | Y ) = P (X)P (Y | X)\nP (Y ) . (2.6)", 
            "title": "3.4 Conditional Probability"
        }, 
        {
            "location": "/foundation_probability/#4-independence-and-conditional-independence", 
            "text": "", 
            "title": "4 Independence and Conditional Independence"
        }, 
        {
            "location": "/foundation_probability/#41-independence", 
            "text": "As we mentioned, we usually expect   P(\u03b1 | \u03b2)   to be different from   P(\u03b1)  . That is, learning that   \u03b2   is true changes our probability over   \u03b1  . However, in some situations equality can occur, so that   P(\u03b1 | \u03b2) = P(\u03b1)  . That is, learning that   \u03b2   occurs did not change our probability of   \u03b1  .  Definition   independent events  We say that an event   \u03b1   is  independent  of event   \u03b2   in   P  ,  denoted    P \\models (\u03b1 \u22a5 \u03b2)  , if   P(\u03b1 | \u03b2) = P(\u03b1)   or if    P(\u03b2) = 0  .  We can also provide an alternative definition for the concept of independence:  Proposition 2.1  A distribution   P   satisfies   (\u03b1 \u22a5 \u03b2)   if and only if   P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2)  .  PROOF Consider first the case where   P(\u03b2) = 0  ; here, we also have   P(\u03b1 \u2229 \u03b2) = 0  , and so the equivalence immediately holds. When   P(\u03b2) \\neq 0  , we can use the chain rule; we write   P(\u03b1\u2229\u03b2) = P(\u03b1 | \u03b2)P(\u03b2)  . Since   \u03b1   is independent of   \u03b2  , we have that   P(\u03b1 | \u03b2) = P(\u03b1)  . Thus,   P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2)  . Conversely, suppose that   P(\u03b1 \u2229 \u03b2) = P(\u03b1)P(\u03b2)  . Then, by definition, we have that    P(\u03b1 | \u03b2) = \\frac{P(\u03b1 \u2229 \u03b2)}{P(\u03b2)} = \\frac{P(\u03b1)P(\u03b2)}{P(\u03b2)} = P(\u03b1).    As an immediate consequence of this alternative definition, we see that independence is a\nsymmetric notion. That is, (\u03b1 \u22a5 \u03b2) implies (\u03b2 \u22a5 \u03b1).\nExample 2.3 For example, suppose that we toss two coins, and let \u03b1 be the event \u201cthe first toss results in a head\u201d\nand \u03b2 the event \u201cthe second toss results in a head.\u201d It is not hard to convince ourselves that we\nexpect that these two events to be independent. Learning that \u03b2 is true would not change our\nprobability of \u03b1. In this case, we see two di\u001berent physical processes (that is, coin tosses) leading\nto the events, which makes it intuitive that the probabilities of the two are independent. In certain\ncases, the same process can lead to independent events. For example, consider the event \u03b1 denoting\n\u201cthe die outcome is even\u201d and the event \u03b2 denoting \u201cthe die outcome is 1 or 2.\u201d It is easy to check\nthat if the die is fair (each of the six possible outcomes has probability 1 6 ), then these two events are\nindependent.", 
            "title": "4.1 Independence"
        }, 
        {
            "location": "/foundation_probability/#42-conditional-independence", 
            "text": "\u0011 While independence is a useful property, it is not often that we encounter two indepen- dent events. A more common situation is when two events are independent given an\nadditional event. For example, suppose we want to reason about the chance that our student\nis accepted to graduate studies at Stanford or MIT. Denote by Stanford the event \u201cadmitted to\nStanford\u201d and by MIT the event \u201cadmitted to MIT.\u201d In most reasonable distributions, these two\nevents are not independent. If we learn that a student was admitted to Stanford, then our\nestimate of her probability of being accepted at MIT is now higher, since it is a sign that she is\na promising student  Now, suppose that both universities base their decisions only on the student\u2019s grade point\naverage (GPA), and we know that our student has a GPA of A. In this case, we might argue\nthat learning that the student was admitted to Stanford should not change the probability that\nshe will be admitted to MIT: Her GPA already tells us the information relevant to her chances\nof admission to MIT, and finding out about her admission to Stanford does not change that.\nFormally, the statement is\nP(MIT | Stanford, GradeA) = P(MIT | GradeA).\nIn this case, we say that MIT is conditionally independent of Stanford given GradeA.\nDefinition 2.3 We say that an event \u03b1 is conditionally independent of event \u03b2 given event \u03b3 in P, denoted\nconditional\nindependence\nP |= (\u03b1 \u22a5 \u03b2 | \u03b3), if P(\u03b1 | \u03b2 \u2229 \u03b3) = P(\u03b1 | \u03b3) or if P(\u03b2 \u2229 \u03b3) = 0.\nIt is easy to extend the arguments we have seen in the case of (unconditional) independencies\nto give an alternative definition.\nProposition 2.2 P satisfies (\u03b1 \u22a5 \u03b2 | \u03b3) if and only if P(\u03b1 \u2229 \u03b2 | \u03b3) = P(\u03b1 | \u03b3)P(\u03b2 | \u03b3).", 
            "title": "4.2 Conditional Independence"
        }, 
        {
            "location": "/foundation_probability/#43-independence-of-random-variables", 
            "text": "Until now, we have focused on independence between events. Thus, we can say that two events,\nsuch as one toss landing heads and a second also landing heads, are independent. However, we\nwould like to say that any pair of outcomes of the coin tosses is independent. To capture such\nstatements, we can examine the generalization of independence to sets of random variables.\nDefinition 2.4 Let X, Y , Z be sets of random variables. We say that X is conditionally independent of Y given\nconditional\nindependence\nZ in a distribution P if P satisfies (X = x \u22a5 Y = y | Z = z) for all values x \u2208 Val(X),\ny \u2208 Val(Y ), and z \u2208 Val(Z). The variables in the set Z are often said to be observed. If the set\nobserved variable Z is empty, then instead of writing (X \u22a5 Y | \u2205), we write (X \u22a5 Y ) and say that X and Y\nare marginally independent.\nmarginal\nindependence\nThus, an independence statement over random variables is a universal quantification over all\npossible values of the random variables.\nThe alternative characterization of conditional independence follows immediately:\nProposition 2.3 The distribution P satisfies (X \u22a5 Y | Z) if and only if P(X, Y | Z) = P(X | Z)P(Y | Z).\nSuppose we learn about a conditional independence. Can we conclude other independence\nproperties that must hold in the distribution? We have already seen one such example:\nsymmetry \u2022 Symmetry:\n(X \u22a5 Y | Z) =\u21d2 (Y \u22a5 X | Z). (2.7)\nThere are several other properties that hold for conditional independence, and that often\nprovide a very clean method for proving important properties about distributions. Some key\nproperties are:  \u2022 Decomposition:\n(X \u22a5 Y , W | Z) =\u21d2 (X \u22a5 Y | Z). (2.8)\nweak union \u2022 Weak union:\n(X \u22a5 Y , W | Z) =\u21d2 (X \u22a5 Y | Z, W). (2.9)\ncontraction \u2022 Contraction:\n(X \u22a5 W | Z, Y ) (X \u22a5 Y | Z) =\u21d2 (X \u22a5 Y , W | Z). (2.10)\nAn additional important property does not hold in general, but it does hold in an important\nsubclass of distributions.\nDefinition 2.5 A distribution P is said to be positive if for all events \u03b1 \u2208 S such that \u03b1 6= \u2205, we have that\npositive\ndistribution\nP(\u03b1)   0.\nFor positive distributions, we also have the following property:\nintersection \u2022 Intersection: For positive distributions, and for mutually disjoint sets X, Y , Z, W :\n(X \u22a5 Y | Z, W) (X \u22a5 W | Z, Y ) =\u21d2 (X \u22a5 Y , W | Z). (2.11)\nThe proof of these properties is not di\u001ecult. For example, to prove Decomposition, assume\nthat (X \u22a5 Y, W | Z) holds. Then, from the definition of conditional independence, we have\nthat P(X, Y, W | Z) = P(X | Z)P(Y, W | Z). Now, using basic rules of probability and\narithmetic, we can show\nP(X, Y | Z) = X\nw\nP(X, Y, w | Z)\n= X\nw\nP(X | Z)P(Y, w | Z)\n= P(X | Z) X\nw\nP(Y, w | Z)\n= P(X | Z)P(Y | Z).\nThe only property we used here is called \u201creasoning by cases\u201d (see exercise 2.6). We conclude\nthat (X \u22a5 Y | Z).", 
            "title": "4.3 Independence of Random Variables"
        }, 
        {
            "location": "/foundation_probability/#5-querying-a-distribution", 
            "text": "Our focus throughout this book is on using a joint probability distribution over multiple random\nvariables to answer queries of interest.", 
            "title": "5 Querying a Distribution"
        }, 
        {
            "location": "/foundation_probability/#51-probability-queries", 
            "text": "probability query Perhaps the most common query type is the probability query. Such a query consists of two\nparts:\nevidence \u2022 The evidence: a subset E of random variables in the model, and an instantiation e to these\nvariables;\nquery variables \u2022 the query variables: a subset Y of random variables in the network.\nOur task is to compute\nP(Y | E = e),\nposterior that is, the posterior probability distribution over the values y of Y , conditioned on the fact that\ndistribution E = e. This expression can also be viewed as the marginal over Y , in the distribution we\nobtain by conditioning on e.", 
            "title": "5.1 Probability Queries"
        }, 
        {
            "location": "/foundation_probability/#52-map-queries", 
            "text": "A second important type of task is that of finding a high-probability joint assignment to some\nsubset of variables. The simplest variant of this type of task is the MAP query (also called\nMAP assignment most probable explanation (MPE)), whose aim is to find the MAP assignment \u2014 the most likely\nassignment to all of the (non-evidence) variables. More precisely, if we let W = X \u2212 E, our\ntask is to find the most likely assignment to the variables in W given the evidence E = e:\nMAP(W | e) = argmax\nw\nP(w, e), (2.12)\nwhere, in general, argmaxx f(x) represents the value of x for which f(x) is maximal. Note\nthat there might be more than one assignment that has the highest posterior probability. In this\ncase, we can either decide that the MAP task is to return the set of possible assignments, or to\nreturn an arbitrary member of that set.\nIt is important to understand the di\u001berence between MAP queries and probability queries. In\na MAP query, we are finding the most likely joint assignment to W . To find the most likely\nassignment to a single variable A, we could simply compute P(A | e) and then pick the most\nlikely value. However, the assignment where each variable individually picks its most\n\u0011 likely value can be quite di\u001berent from the most likely joint assignment to all variables\nsimultaneously. This phenomenon can occur even in the simplest case, where we have no\nevidence.\nExample 2.4 Consider a two node chain A \u2192 B where A and B are both binary-valued. Assume that:\na0 a1\n0.4 0.6\nA b0 b1\na0 0.1 0.9\na1 0.5 0.5\n(2.13)\nWe can see that P(a1)   P(a0), so that MAP(A) = a1. However, MAP(A, B) = (a0, b1): Both\nvalues of B have the same probability given a1. Thus, the most likely assignment containing a1 has\nprobability 0.6 \u00d7 0.5 = 0.3. On the other hand, the distribution over values of B is more skewed\ngiven a0, and the most likely assignment (a0, b1) has the probability 0.4 \u00d7 0.9 = 0.36. Thus, we\nhave that argmaxa,b P(a, b) 6= (argmaxa P(a),argmaxb P(b)).", 
            "title": "5.2 MAP Queries"
        }, 
        {
            "location": "/foundation_probability/#53-marginal-map-queries", 
            "text": "To motivate our second query type, let us return to the phenomenon demonstrated in example 2.4. Now, consider a medical diagnosis problem, where the most likely disease has multiple\npossible symptoms, each of which occurs with some probability, but not an overwhelming probability. On the other hand, a somewhat rarer disease might have only a few symptoms, each\nof which is very likely given the disease. As in our simple example, the MAP assignment to\nthe data and the symptoms might be higher for the second disease than for the first one. The\nsolution here is to look for the most likely assignment to the disease variable(s) only, rather than\nthe most likely assignment to both the disease and symptom variables. This approach suggests\nmarginal MAP the use of a more general query type. In the marginal MAP query, we have a subset of variables\nY that forms our query. The task is to find the most likely assignment to the variables in Y\ngiven the evidence E = e:\nMAP(Y | e) = arg max\ny\nP(y | e).\nIf we let Z = X \u2212 Y \u2212 E, the marginal MAP task is to compute:\nMAP(Y | e) = arg max\nY\nX Z\nP(Y , Z | e).\nThus, marginal MAP queries contain both summations and maximizations; in a way, it contains\nelements of both a conditional probability query and a MAP query.\nNote that example 2.4 shows that marginal MAP assignments are not monotonic: the most\nlikely assignment MAP(Y1 | e) might be completely di\u001berent from the assignment to Y1 in\nMAP({Y1, Y2} | e). Thus, in particular, we cannot use a MAP query to give us the correct\nanswer to a marginal MAP query.", 
            "title": "5.3 Marginal MAP Queries"
        }, 
        {
            "location": "/foundation_probability/#6-continuous-spaces", 
            "text": "In the previous section, we focused on random variables that have a finite set of possible values.\nIn many situations, we also want to reason about continuous quantities such as weight, height,\nduration, or cost that take real numbers in IR.\nWhen dealing with probabilities over continuous random variables, we have to deal with some\ntechnical issues. For example, suppose that we want to reason about a random variable X that\ncan take values in the range between 0 and 1. That is, Val(X) is the interval [0, 1]. Moreover,\nassume that we want to assign each number in this range equal probability. What would be the\nprobability of a number x? Clearly, since each x has the same probability, and there are infinite\nnumber of values, we must have that P(X = x) = 0. This problem appears even if we do not\nrequire uniform probability.", 
            "title": "6 Continuous Spaces"
        }, 
        {
            "location": "/foundation_probability/#61-probability-density-functions", 
            "text": "How do we define probability over a continuous random variable? We say that a function\ndensity function p : IR 7\u2192 IR is a probability density function or (PDF) for X if it is a nonnegative integrable  function such that\nZ\nVal(X)\np(x)dx = 1.\nThat is, the integral over the set of possible values of X is 1. The PDF defines a distribution for\nX as follows: for any x in our event space:\nP(X \u2264 a) =\naZ\n\u2212\u221e\np(x)dx.\ncumulative The function P is the cumulative distribution for X. We can easily employ the rules of\ndistribution probability to see that by using the density function we can evaluate the probability of other\nevents. For example,\nP(a \u2264 X \u2264 b) =\nbZa\np(x)dx.\nIntuitively, the value of a PDF p(x) at a point x is the incremental amount that x adds to the\ncumulative distribution in the integration process. The higher the value of p at and around x,\nthe more mass is added to the cumulative distribution as it passes x.\nThe simplest PDF is the uniform distribution.\nDefinition 2.6 A variable X has a uniform distribution over [a, b], denoted X \u223c Unif[a,b] if it has the PDF\nuniform\ndistribution\np(x) = \u001a 0 otherwise b\u2212 1a b \u2265 x \u2265 a.\nThus, the probability of any subinterval of [a, b] is proportional its size relative to the size of\n[a, b]. Note that, if b \u2212 a   1, then the density can be greater than 1. Although this looks\nunintuitive, this situation can occur even in a legal PDF, if the interval over which the value is\ngreater than 1 is not too large. We have only to satisfy the constraint that the total area under\nthe PDF is 1.\nAs a more complex example, consider the Gaussian distribution.\nDefinition 2.7 A random variable X has a Gaussian distribution with mean \u00b5 and variance \u03c32, denoted X \u223c\nGaussian\ndistribution\nN \u00b5; \u03c32\u0001, if it has the PDF\np(x) = \u221a21\u03c0\u03c3 e\u2212 (x2 \u2212 \u03c3\u00b5 2)2 .\nstandard A standard Gaussian is one with mean 0 and variance 1.\nGaussian\nA Gaussian distribution has a bell-like curve, where the mean parameter \u00b5 controls the\nlocation of the peak, that is, the value for which the Gaussian gets its maximum value. The\nvariance parameter \u03c32 determines how peaked the Gaussian is: the smaller the variance, the     more peaked the Gaussian. Figure 2.2 shows the probability density function of a few di\u001berent\nGaussian distributions.\nMore technically, the probability density function is specified as an exponential, where the\nexpression in the exponent corresponds to the square of the number of standard deviations \u03c3\nthat x is away from the mean \u00b5. The probability of x decreases exponentially with the square\nof its deviation from the mean, as measured in units of its standard deviation.", 
            "title": "6.1 Probability Density Functions"
        }, 
        {
            "location": "/foundation_probability/#62-joint-density-functions", 
            "text": "The discussion of density functions for a single variable naturally extends for joint distributions\nof continuous random variables.\nDefinition 2.8 Let P be a joint distribution over continuous random variables X1, . . . , Xn. A function p(x1, . . . , xn)\njoint density is a joint density function of X1, . . . , Xn if\n\u2022 p(x1, . . . , xn) \u2265 0 for all values x1, . . . , xn of X1, . . . , Xn.\n\u2022 p is an integrable function.\n\u2022 For any choice of a1, . . . , an, and b1, . . . , bn,\nP(a1 \u2264 X1 \u2264 b1, . . . , an \u2264 Xn \u2264 bn) =\nb1\nZa1\n\u00b7 \u00b7 \u00b7\nb\nnZ\na\nn\np(x1, . . . , xn)dx1 . . . dxn.\nThus, a joint density specifies the probability of any joint event over the variables of interest.\nBoth the uniform distribution and the Gaussian distribution have natural extensions to the\nmultivariate case. The definition of a multivariate uniform distribution is straightforward. We\ndefer the definition of the multivariate Gaussian to section 7.1.\nFrom the joint density we can derive the marginal density of any random variable by integrating out the other variables. Thus, for example, if p(x, y) is the joint density of X and Y  then\np(x) =\n\u221e Z\n\u2212\u221e\np(x, y)dy.\nTo see why this equality holds, note that the event a \u2264 X \u2264 b is, by definition, equal to the\nevent \u201ca \u2264 X \u2264 b and \u2212\u221e \u2264 Y \u2264 \u221e.\u201d This rule is the direct analogue of marginalization for\ndiscrete variables. Note that, as with discrete probability distributions, we abuse notation a bit\nand use p to denote both the joint density of X and Y and the marginal density of X. In cases\nwhere the distinction is not clear, we use subscripts, so that pX will be the marginal density, of\nX, and pX,Y the joint density.", 
            "title": "6.2 Joint Density Functions"
        }, 
        {
            "location": "/foundation_probability/#63-conditional-density-functions", 
            "text": "As with discrete random variables, we want to be able to describe conditional distributions of\ncontinuous variables. Suppose, for example, we want to define P(Y | X = x). Applying the\ndefinition of conditional distribution (equation (2.1)), we run into a problem, since P(X = x) =\n0. Thus, the ratio of P(Y, X = x) and P(X = x) is undefined.\nTo avoid this problem, we might consider conditioning on the event x \u2212 \u000f \u2264 X \u2264 x + \u000f,\nwhich can have a positive probability. Now, the conditional probability is well defined. Thus, we\nmight consider the limit of this quantity when \u000f \u2192 0. We define\nP(Y | x) = lim\n\u000f\u21920\nP(Y | x \u2212 \u000f \u2264 X \u2264 x + \u000f).\nWhen does this limit exist? If there is a continuous joint density function p(x, y), then we can\nderive the form for this term. To do so, consider some event on Y , say a \u2264 Y \u2264 b. Recall that\nP(a \u2264 Y \u2264 b | x \u2212 \u000f \u2264 X \u2264 x + \u000f) = P(a \u2264 Y \u2264 b, x \u2212 \u000f \u2264 X \u2264 x + \u000f)\nP(x \u2212 \u000f \u2264 X \u2264 x + \u000f)\n=\nRa b Rx x\u2212 +\u000f\u000f p(x0, y)dydx0\nRx x\u2212 +\u000f\u000f p(x0)dx0 .\nWhen \u000f is su\u001eciently small, we can approximate\nx+\u000f\nZ\nx\u2212\u000f\np(x0)dx0 \u2248 2\u000fp(x).\nUsing a similar approximation for p(x0, y), we get\nP(a \u2264 Y \u2264 b | x \u2212 \u000f \u2264 X \u2264 x + \u000f) \u2248\nRa b 2\u000fp(x, y)dy\n2\u000fp(x)\n=\nbZa\np(x, y)\np(x) dy.\nWe conclude that p(x,y)\np(x) is the density of P(Y | X = x).  Let p(x, y) be the joint density of X and Y . The conditional density function of Y given X is\nconditional\ndensity function\ndefined as\np(y | x) = p(x, y)\np(x)\nWhen p(x) = 0, the conditional density is undefined.\nThe conditional density p(y | x) characterizes the conditional distribution P(Y | X = x) we\ndefined earlier.\nThe properties of joint distributions and conditional distributions carry over to joint and\nconditional density functions. In particular, we have the chain rule\np(x, y) = p(x)p(y | x) (2.14)\nand Bayes\u2019 rule\np(x | y) = p(x)p(y | x)\np(y) . (2.15)\nAs a general statement, whenever we discuss joint distributions of continuous random variables, we discuss properties with respect to the joint density function instead of the joint\ndistribution, as we do in the case of discrete variables. Of particular interest is the notion of\n(conditional) independence of continuous random variables.\nDefinition 2.10 Let X, Y , and Z be sets of continuous random variables with joint density p(X, Y , Z). We say\nconditional that X is conditionally independent of Y given Z if\nindependence\np(x | z) = p(x | y, z) for all x, y, z such that p(z)   0.", 
            "title": "6.3 Conditional Density Functions"
        }, 
        {
            "location": "/foundation_probability/#7-expectation-and-variance", 
            "text": "", 
            "title": "7 Expectation and Variance"
        }, 
        {
            "location": "/foundation_probability/#71-expectation", 
            "text": "expectation Let X be a discrete random variable that takes numerical values; then the expectation of X\nunder the distribution P is\nIEP[X] = X\nx\nx \u00b7 P(x).\nIf X is a continuous variable, then we use the density function\nIEP[X] = Z x \u00b7 p(x)dx.\nFor example, if we consider X to be the outcome of rolling a fair die with probability 1/6\nfor each outcome, then IE[X] = 1 \u00b7 1 6 + 2 \u00b7 1 6 + \u00b7 \u00b7 \u00b7 + 6 \u00b7 1 6 = 3.5. On the other hand, if\nwe consider a biased die where P(X = 6) = 0.5 and P(X = x) = 0.1 for x   6, then\nIE[X] = 1 \u00b7 0.1 + \u00b7 \u00b7 \u00b7 + 5 \u00b7 0.1 + \u00b7 \u00b7 \u00b7 + 6 \u00b7 0.5 = 4.5.  Often we are interested in expectations of a function of a random variable (or several random\nvariables). Thus, we might consider extending the definition to consider the expectation of a\nfunctional term such as X2 + 0.5X. Note, however, that any function g of a set of random\nvariables X1, . . . , Xk is essentially defining a new random variable Y : For any outcome \u03c9 \u2208 \u2126,\nwe define the value of Y as g(fX1(\u03c9), . . . , fXk(\u03c9)).\nBased on this discussion, we often define new random variables by a functional term. For\nexample Y = X2, or Y = eX. We can also consider functions that map values of one or more\ncategorical random variables to numerical values. One such function that we use quite often is\nindicator function the indicator function, which we denote 11{X = x}. This function takes value 1 when X = x,\nand 0 otherwise.\nIn addition, we often consider expectations of functions of random variables without bothering\nto name the random variables they define. For example IEP [X + Y ]. Nonetheless, we should\nkeep in mind that such a term does refer to an expectation of a random variable.\nWe now turn to examine properties of the expectation of a random variable.\nFirst, as can be easily seen, the expectation of a random variable is a linear function in that\nrandom variable. Thus,\nIE[a \u00b7 X + b] = aIE[X] + b.\nA more complex situation is when we consider the expectation of a function of several random\nvariables that have some joint behavior. An important property of expectation is that the\nexpectation of a sum of two random variables is the sum of the expectations.\nProposition 2.4 IE[X + Y ] = IE[X] + IE[Y ].\nlinearity of This property is called linearity of expectation. It is important to stress that this identity is true\nexpectation even when the variables are not independent. As we will see, this property is key in simplifying\nmany seemingly complex problems.\nFinally, what can we say about the expectation of a product of two random variables? In\ngeneral, very little:\nExample 2.5 Consider two random variables X and Y , each of which takes the value +1 with probability 1/2,\nand the value \u22121 with probability 1/2. If X and Y are independent, then IE[X \u00b7 Y ] = 0. On the\nother hand, if X and Y are correlated in that they always take the same value, then IE[X \u00b7 Y ] = 1.\nHowever, when X and Y are independent, then, as in our example, we can compute the\nexpectation simply as a product of their individual expectations:\nProposition 2.5 If X and Y are independent, then\nIE[X \u00b7 Y ] = IE[X] \u00b7 IE[Y ].\nconditional We often also use the expectation given some evidence. The conditional expectation of X\nexpectation given y is\nIEP [X | y] = X\nx\nx \u00b7 P(x | y).", 
            "title": "7.1 Expectation"
        }, 
        {
            "location": "/foundation_probability/#72-variance", 
            "text": "The expectation of X tells us the mean value of X. However, It does not indicate how far X\nvariance deviates from this value. A measure of this deviation is the variance of X.\nVVarP [X] = IEP h(X \u2212 IEP [X])2i.\nThus, the variance is the expectation of the squared di\u001berence between X and its expected\nvalue. It gives us an indication of the spread of values of X around the expected value.\nAn alternative formulation of the variance is\nVVar[X] = IEX2 \u2212 (IE[X])2 . (2.16)\n(see exercise 2.11).\nSimilar to the expectation, we can consider the expectation of a functions of random variables.\nProposition 2.6 If X and Y are independent, then\nVVar[X + Y ] = VVar[X] + VVar[Y ].\nIt is straightforward to show that the variance scales as a quadratic function of X. In\nparticular, we have:\nVVar[a \u00b7 X + b] = a2VVar[X].\nFor this reason, we are often interested in the square root of the variance, which is called the\nstandard standard deviation of the random variable. We define\ndeviation\n\u03c3X = pVVar[X].\nThe intuition is that it is improbable to encounter values of X that are farther than several\nstandard deviations from the expected value of X. Thus, \u03c3X is a normalized measure of\n\u201cdistance\u201d from the expected value of X.\nAs an example consider the Gaussian distribution of definition 2.7.\nProposition 2.7 Let X be a random variable with Gaussian distribution N(\u00b5, \u03c32), then IE[X] = \u00b5 and VVar[X] =\n\u03c32.\nThus, the parameters of the Gaussian distribution specify the expectation and the variance of\nthe distribution. As we can see from the form of the distribution, the density of values of X\ndrops exponentially fast in the distance x\u2212\u00b5\n\u03c3\n.\nNot all distributions show such a rapid decline in the probability of outcomes that are distant\nfrom the expectation. However, even for arbitrary distributions, one can show that there is a\ndecline.\nTheorem 2.1 (Chebyshev inequality):\nChebyshev\u2019s\ninequality\nP (|X \u2212 IEP [X]| \u2265 t) \u2264 VVarP [X]\nt2 .  We can restate this inequality in terms of standard deviations: We write t = k\u03c3X to get\nP(|X \u2212 IEP [X]| \u2265 k\u03c3X) \u2264 1\nk2.\nThus, for example, the probability of X being more than two standard deviations away from\nIE[X] is less than 1/4.", 
            "title": "7.2 Variance"
        }, 
        {
            "location": "/foundation_graph/", 
            "text": "Foundation: Graph\n\n\nPerhaps the most pervasive concept in this book is the representation of a probability distribution\nusing a graph as a data structure. In this section, we survey some of the basic concepts in graph\ntheory used in the book.\n\n\n1 Nodes and Edges\n\n\nA graph is a data structure K consisting of a set of nodes and a set of edges. Throughout most\nthis book, we will assume that the set of nodes is X = {X1,...,Xn}. A pair of nodes Xi,Xj\ndirected edge can be connected by a directed edge Xi \u2192 Xj or an undirected edge Xi\u2014Xj. Thus, the set\nundirected edge of edges E is a set of pairs, where each pair is one of Xi \u2192 Xj, Xj \u2192 Xi, or Xi\u2014Xj, for\nXi,Xj \u2208 X , i \n j. We assume throughout the book that, for each pair of nodes Xi,Xj, at\nmost one type of edge exists; thus, we cannot have both Xi \u2192 Xj and Xj \u2192 Xi, nor can\nwe have Xi \u2192 Xj and Xi\u2014Xj.2 The notation Xi \u2190 Xj is equivalent to Xj \u2192 Xi, and the\nnotation X\nj\u2014Xi is equivalent to Xi\u2014Xj. We use Xi\n Xj to represent the case where Xi\nand X\nj are connected via some edge, whether directed (in any direction) or undirected.\nIn many cases, we want to restrict attention to graphs that contain only edges of one kind\ndirected graph or another. We say that a graph is directed if all edges are either Xi \u2192 Xj or Xj \u2192 Xi. We\nusually denote directed graphs as G. We say that a graph is undirected if all edges are Xi\u2014Xj.\nundirected graph\nWe denote undirected graphs as H. We sometimes convert a general graph to an undirected\ngraph by ignoring the directions on the edges.\nDefinition 2.11 Given a graph K = (X, E), its undirected version is a graph H = (X, E0) where E0 = {X\u2014Y :\ngraph\u2019s\nundirected\nversion\nX\n Y \u2208 E}.\nWhenever we have that Xi \u2192 Xj \u2208 E, we say that Xj is the child of Xi in K, and that\nchild Xi is the parent of Xj in K. When we have Xi\u2014Xj \u2208 E, we say that Xi is a neighbor of\nparent\nneighbor\nXj\nin K (and vice versa). We say that X and Y are adjacent whenever X\n Y \u2208 E. We use\nPaX to denote the parents of X, ChX to denote its children, and NbX to denote its neighbors.\nWe define the boundary of X, denoted BoundaryX, to be PaX \u222a NbX; for DAGs, this set is\nboundary simply X\u2019s parents, and for undirected graphs X\u2019s neighbors.3 Figure 2.3 shows an example of\na graph K. There, we have that A is the only parent of C, and F,I are the children of C. The\ndegree only neighbor of C is D, but its adjacent nodes are A,D,F,I. The degree of a node X is the\nnumber of edges in which it participates. Its indegree is the number of directed edges Y \u2192 X.\nindegree\nThe degree of a graph is the maximal degree of a node in the graph.\n2. Note that our definition is somewhat restricted, in that it disallows cycles of length two, where Xi \u2192 Xj \u2192 Xi,\nand allows self-loops where Xi \u2192 Xi.\n3. When the graph is not clear from context, we often add the graph as an additional argument.\n\n\n\n\n\n\n\n\n2 Subgraphs\n\n\nIn many cases, we want to consider only the part of the graph that is associated with a particular\nsubset of the nodes.\nDefinition 2.12 Let K = (X , E), and let X \u2282 X . We define the induced subgraph K[X] to be the graph (X, E0)\ninduced\nsubgraph\nwhere E0 are all the edges X\n Y \u2208 E such that X, Y \u2208 X.\nFor example, figure 2.4a shows the induced subgraph K[C, D, I].\nA type of subgraph that is often of particular interest is one that contains all possible edges.\nDefinition 2.13 A subgraph over X is complete if every two nodes in X are connected by some edge. The set X\ncomplete\nsubgraph\nis often called a clique; we say that a clique X is maximal if for any superset of nodes Y \u2283 X,\nclique\nY is not a clique.\nAlthough the subset of nodes X can be arbitrary, we are often interested in sets of nodes\nthat preserve certain aspects of the graph structure.\nDefinition 2.14 We say that a subset of nodes X \u2208 X is upwardly closed in K if, for any X \u2208 X, we have that\nupward closure BoundaryX \u2282 X. We define the upward closure of X to be the minimal upwardly closed subset\n\n\nY that contains X. We define the upwardly closed subgraph of X, denoted K+[X], to be the\ninduced subgraph over Y , K[Y ].\nFor example, the set A, B, C, D, E is the upward closure of the set {C} in K. The upwardly\nclosed subgraph of {C} is shown in figure 2.4b. The upwardly closed subgraph of {C, D, I} is\nshown in figure 2.4c.\n\n\n3 Paths and Trails\n\n\nUsing the basic notion of edges, we can define di\u001berent types of longer-range connections in the graph.\n\n\nDefinition\n \npath\n\n\nWe say that \n X_1, . . . , X_k \n form a path in the graph \n K = (X, E) \n  if, for every \n i = 1, . . . , k \u2212 1 \n,\nwe have that either \n X_i \u2192 X_{i+1} \n or \n X_{i} - X_{i+1} \n. A path is directed if, for at least one i, we have \n X_{i} \u2192 X_{i+1} \n.\n\n\nDefinition\n \ntrail\n\n\nWe say that \n X_1, ... , X_k \n form a trail in the graph \n K = (X, E) \n if, for every \n i = 1, . . . , k \u2212 1 \n, we\n have that \n X_i \\rightleftharpoons X_{i+1} \n.\n\n\n\n\n\n\n\n\nIn the graph \n K \n of figure 2.3, \n  A, C, D, E, I \n is a path, and hence also a trail. On the other hand, \n A, C, F, G, D \n is a trail, which is not a path.\n\n\nDefinition\n \nconnected graph\n\n\nA graph\n is connected if \nfor every\n \n X_i, X_j \n there is a trail between \n X_i \n and \n X_j \n.\n\n\nWe can now define longer-range relationships in the graph.\n\n\nDefinition\n \nancestor\n, \ndescendant\n\n\nWe say that \n X \n is an ancestor of \n Y \n in \n K = (X, E) \n, and that \n Y \n is a descendant of \n X \n, if there\nexists a directed path \n X_1, ..., X_k \n with \n X_1 = X \n and \n X_k = Y \n. We use \n Descendants_X \n to denote\n\nX\u2019s descendants\n, \n Ancestors_X \n to denote \nX\u2019s ancestors\n, and \n NonDescendants_X \n to denote the set of\nnodes in \n X \u2212 Descendants_X \n.\n\n\nIn our example graph K, we have that \n F, G, I \n are descendants of \n C \n. The ancestors of \n C \n are \n A \n, via the path \n A, C, \n and \n B \n, via the path \n B, E, D, C \n.\n\n\nA final useful notion is that of an ordering of the nodes in a directed graph that is consistent with the directionality its edges.\n\n\nDefinition\n \ntopological ordering\n\n\nLet \n G = (X, E) \n be a graph. An ordering of the nodes \n X_1, ... , X_n \n is a topological ordering relative\nto \n K \n if, whenever we have \n Xi \u2192 Xj \u2208 E \n, then \n i < j \n.\n\n\nAppendix A.3.1 presents an algorithm for finding such a topological ordering.\n\n\n4 Cycles and Loops\n\n\nNote that, in general, we can have a cyclic path that leads from a node to itself, making that\nnode its own descendant.\n\n\nDefinition 2.20 A cycle in K is a directed path X1, . . . , Xk where X1 = Xk. A graph is acyclic if it contains no\ncycle\nacyclic\ncycles.\nFor most of this book, we will restrict attention to graphs that do not allow such cycles, since it\nis quite di\u001ecult to define a coherent probabilistic model over graphs with directed cycles.\nDAG A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the\nbasic graphical representation that underlies Bayesian networks. For some of this book, we also\nuse acyclic graphs that are partially directed. The graph K of figure 2.3 is acyclic. However, if\nwe add the undirected edge A\u2014E to K, we have a path A, C, D, E, A from A to itself. Clearly,\nadding a directed edge E \u2192 A would also lead to a cycle. Note that prohibiting cycles does\nnot imply that there is no trail from a node to itself. For example, K contains several trails:\nC, D, E, I, C as well as C, D, G, F, C.\nAn acyclic graph containing both directed and undirected edges is called a partially directed\nPDAG acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be\nchain component decomposed into a directed graph of chain components, where the nodes within each chain\ncomponent are connected to each other only with undirected edges. The acyclicity of a PDAG\nguarantees us that we can order the components so that all edges point from lower-numbered\ncomponents to higher-numbered ones.\nDefinition 2.21 Let K be a PDAG over X . Let K1, . . . , K` be a disjoint partition of X such that:\n\u2022 the induced subgraph over Ki contains no directed edges;\n\u2022 for any pair of nodes X \u2208 Ki and Y \u2208 Kj for i \n j, an edge between X and Y can only\nbe a directed edge X \u2192 Y .\nchain component Each component Ki is called a chain component.\nchain graph Because of its chain structure, a PDAG is also called a chain graph.\nExample 2.6 In the PDAG of figure 2.3, we have six chain components: {A}, {B}, {C, D, E}, {F, G}, {H},\nand {I}. This ordering of the chain components is one of several possible legal orderings.\nNote that when the PDAG is an undirected graph, the entire graph forms a single chain\ncomponent. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node\nin the graph is its own chain component.\n\n\n\n\n\n\n\n\nDi\u001berent from a cycle is the notion of a loop:\nDefinition 2.22 A loop in K is a trail X1, . . . , Xk where X1 = Xk. A graph is singly connected if it contains\nloop\nsingly connected\nno loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node.\nleaf\nA singly connected directed graph is also called a polytree. A singly connected undirected graph is\npolytree\ncalled a forest; if it is also connected, it is called a tree.\nforest\ntree\nWe can also define a notion of a forest, or of a tree, for directed graphs.\nDefinition 2.23\nA directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is\nalso connected.\nNote that polytrees are very di\u001berent from trees. For example, figure 2.5 shows a graph that is a\npolytree but is not a tree, because several nodes have more than one parent. As we will discuss\nlater in the book, loops in the graph increase the computational cost of various tasks.\nWe conclude this section with a final definition relating to loops in the graph. This definition\nwill play an important role in evaluating the cost of reasoning using graph-based representations.\nDefinition 2.24 Let X1\u2014X2\u2014 \u00b7 \u00b7 \u00b7 \u2014Xk\u2014X1 be a loop in the graph; a chord in the loop is an edge connecting\nchordal graph Xi and Xj for two nonconsecutive nodes Xi, Xj. An undirected graph H is said to be chordal if\nany loop X1\u2014X2\u2014 \u00b7 \u00b7 \u00b7 \u2014Xk\u2014X1 for k \u2265 4 has a chord.\nThus, for example, a loop A\u2014B\u2014C\u2014D\u2014A (as in figure 1.1b) is nonchordal, but adding an\nedge A\u2014C would render it chordal. In other words, in a chordal graph, the longest \u201cminimal\nloop\u201d (one that has no shortcut) is a triangle. Thus, chordal graphs are often also called\ntriangulated triangulated.\ngraph We can extend the notion of chordal graphs to graphs that contain directed edges.\nDefinition 2.25 A graph K is said to be chordal if its underlying undirected graph is chordal.", 
            "title": "Foundation: Graph"
        }, 
        {
            "location": "/foundation_graph/#foundation-graph", 
            "text": "Perhaps the most pervasive concept in this book is the representation of a probability distribution\nusing a graph as a data structure. In this section, we survey some of the basic concepts in graph\ntheory used in the book.", 
            "title": "Foundation: Graph"
        }, 
        {
            "location": "/foundation_graph/#1-nodes-and-edges", 
            "text": "A graph is a data structure K consisting of a set of nodes and a set of edges. Throughout most\nthis book, we will assume that the set of nodes is X = {X1,...,Xn}. A pair of nodes Xi,Xj\ndirected edge can be connected by a directed edge Xi \u2192 Xj or an undirected edge Xi\u2014Xj. Thus, the set\nundirected edge of edges E is a set of pairs, where each pair is one of Xi \u2192 Xj, Xj \u2192 Xi, or Xi\u2014Xj, for\nXi,Xj \u2208 X , i   j. We assume throughout the book that, for each pair of nodes Xi,Xj, at\nmost one type of edge exists; thus, we cannot have both Xi \u2192 Xj and Xj \u2192 Xi, nor can\nwe have Xi \u2192 Xj and Xi\u2014Xj.2 The notation Xi \u2190 Xj is equivalent to Xj \u2192 Xi, and the\nnotation X\nj\u2014Xi is equivalent to Xi\u2014Xj. We use Xi\n Xj to represent the case where Xi\nand X\nj are connected via some edge, whether directed (in any direction) or undirected.\nIn many cases, we want to restrict attention to graphs that contain only edges of one kind\ndirected graph or another. We say that a graph is directed if all edges are either Xi \u2192 Xj or Xj \u2192 Xi. We\nusually denote directed graphs as G. We say that a graph is undirected if all edges are Xi\u2014Xj.\nundirected graph\nWe denote undirected graphs as H. We sometimes convert a general graph to an undirected\ngraph by ignoring the directions on the edges.\nDefinition 2.11 Given a graph K = (X, E), its undirected version is a graph H = (X, E0) where E0 = {X\u2014Y :\ngraph\u2019s\nundirected\nversion\nX\n Y \u2208 E}.\nWhenever we have that Xi \u2192 Xj \u2208 E, we say that Xj is the child of Xi in K, and that\nchild Xi is the parent of Xj in K. When we have Xi\u2014Xj \u2208 E, we say that Xi is a neighbor of\nparent\nneighbor\nXj\nin K (and vice versa). We say that X and Y are adjacent whenever X\n Y \u2208 E. We use\nPaX to denote the parents of X, ChX to denote its children, and NbX to denote its neighbors.\nWe define the boundary of X, denoted BoundaryX, to be PaX \u222a NbX; for DAGs, this set is\nboundary simply X\u2019s parents, and for undirected graphs X\u2019s neighbors.3 Figure 2.3 shows an example of\na graph K. There, we have that A is the only parent of C, and F,I are the children of C. The\ndegree only neighbor of C is D, but its adjacent nodes are A,D,F,I. The degree of a node X is the\nnumber of edges in which it participates. Its indegree is the number of directed edges Y \u2192 X.\nindegree\nThe degree of a graph is the maximal degree of a node in the graph.\n2. Note that our definition is somewhat restricted, in that it disallows cycles of length two, where Xi \u2192 Xj \u2192 Xi,\nand allows self-loops where Xi \u2192 Xi.\n3. When the graph is not clear from context, we often add the graph as an additional argument.", 
            "title": "1 Nodes and Edges"
        }, 
        {
            "location": "/foundation_graph/#2-subgraphs", 
            "text": "In many cases, we want to consider only the part of the graph that is associated with a particular\nsubset of the nodes.\nDefinition 2.12 Let K = (X , E), and let X \u2282 X . We define the induced subgraph K[X] to be the graph (X, E0)\ninduced\nsubgraph\nwhere E0 are all the edges X\n Y \u2208 E such that X, Y \u2208 X.\nFor example, figure 2.4a shows the induced subgraph K[C, D, I].\nA type of subgraph that is often of particular interest is one that contains all possible edges.\nDefinition 2.13 A subgraph over X is complete if every two nodes in X are connected by some edge. The set X\ncomplete\nsubgraph\nis often called a clique; we say that a clique X is maximal if for any superset of nodes Y \u2283 X,\nclique\nY is not a clique.\nAlthough the subset of nodes X can be arbitrary, we are often interested in sets of nodes\nthat preserve certain aspects of the graph structure.\nDefinition 2.14 We say that a subset of nodes X \u2208 X is upwardly closed in K if, for any X \u2208 X, we have that\nupward closure BoundaryX \u2282 X. We define the upward closure of X to be the minimal upwardly closed subset  Y that contains X. We define the upwardly closed subgraph of X, denoted K+[X], to be the\ninduced subgraph over Y , K[Y ].\nFor example, the set A, B, C, D, E is the upward closure of the set {C} in K. The upwardly\nclosed subgraph of {C} is shown in figure 2.4b. The upwardly closed subgraph of {C, D, I} is\nshown in figure 2.4c.", 
            "title": "2 Subgraphs"
        }, 
        {
            "location": "/foundation_graph/#3-paths-and-trails", 
            "text": "Using the basic notion of edges, we can define di\u001berent types of longer-range connections in the graph.  Definition   path  We say that   X_1, . . . , X_k   form a path in the graph   K = (X, E)    if, for every   i = 1, . . . , k \u2212 1  ,\nwe have that either   X_i \u2192 X_{i+1}   or   X_{i} - X_{i+1}  . A path is directed if, for at least one i, we have   X_{i} \u2192 X_{i+1}  .  Definition   trail  We say that   X_1, ... , X_k   form a trail in the graph   K = (X, E)   if, for every   i = 1, . . . , k \u2212 1  , we\n have that   X_i \\rightleftharpoons X_{i+1}  .     In the graph   K   of figure 2.3,    A, C, D, E, I   is a path, and hence also a trail. On the other hand,   A, C, F, G, D   is a trail, which is not a path.  Definition   connected graph  A graph  is connected if  for every    X_i, X_j   there is a trail between   X_i   and   X_j  .  We can now define longer-range relationships in the graph.  Definition   ancestor ,  descendant  We say that   X   is an ancestor of   Y   in   K = (X, E)  , and that   Y   is a descendant of   X  , if there\nexists a directed path   X_1, ..., X_k   with   X_1 = X   and   X_k = Y  . We use   Descendants_X   to denote X\u2019s descendants ,   Ancestors_X   to denote  X\u2019s ancestors , and   NonDescendants_X   to denote the set of\nnodes in   X \u2212 Descendants_X  .  In our example graph K, we have that   F, G, I   are descendants of   C  . The ancestors of   C   are   A  , via the path   A, C,   and   B  , via the path   B, E, D, C  .  A final useful notion is that of an ordering of the nodes in a directed graph that is consistent with the directionality its edges.  Definition   topological ordering  Let   G = (X, E)   be a graph. An ordering of the nodes   X_1, ... , X_n   is a topological ordering relative\nto   K   if, whenever we have   Xi \u2192 Xj \u2208 E  , then   i < j  .  Appendix A.3.1 presents an algorithm for finding such a topological ordering.", 
            "title": "3 Paths and Trails"
        }, 
        {
            "location": "/foundation_graph/#4-cycles-and-loops", 
            "text": "Note that, in general, we can have a cyclic path that leads from a node to itself, making that\nnode its own descendant.  Definition 2.20 A cycle in K is a directed path X1, . . . , Xk where X1 = Xk. A graph is acyclic if it contains no\ncycle\nacyclic\ncycles.\nFor most of this book, we will restrict attention to graphs that do not allow such cycles, since it\nis quite di\u001ecult to define a coherent probabilistic model over graphs with directed cycles.\nDAG A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the\nbasic graphical representation that underlies Bayesian networks. For some of this book, we also\nuse acyclic graphs that are partially directed. The graph K of figure 2.3 is acyclic. However, if\nwe add the undirected edge A\u2014E to K, we have a path A, C, D, E, A from A to itself. Clearly,\nadding a directed edge E \u2192 A would also lead to a cycle. Note that prohibiting cycles does\nnot imply that there is no trail from a node to itself. For example, K contains several trails:\nC, D, E, I, C as well as C, D, G, F, C.\nAn acyclic graph containing both directed and undirected edges is called a partially directed\nPDAG acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be\nchain component decomposed into a directed graph of chain components, where the nodes within each chain\ncomponent are connected to each other only with undirected edges. The acyclicity of a PDAG\nguarantees us that we can order the components so that all edges point from lower-numbered\ncomponents to higher-numbered ones.\nDefinition 2.21 Let K be a PDAG over X . Let K1, . . . , K` be a disjoint partition of X such that:\n\u2022 the induced subgraph over Ki contains no directed edges;\n\u2022 for any pair of nodes X \u2208 Ki and Y \u2208 Kj for i   j, an edge between X and Y can only\nbe a directed edge X \u2192 Y .\nchain component Each component Ki is called a chain component.\nchain graph Because of its chain structure, a PDAG is also called a chain graph.\nExample 2.6 In the PDAG of figure 2.3, we have six chain components: {A}, {B}, {C, D, E}, {F, G}, {H},\nand {I}. This ordering of the chain components is one of several possible legal orderings.\nNote that when the PDAG is an undirected graph, the entire graph forms a single chain\ncomponent. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node\nin the graph is its own chain component.     Di\u001berent from a cycle is the notion of a loop:\nDefinition 2.22 A loop in K is a trail X1, . . . , Xk where X1 = Xk. A graph is singly connected if it contains\nloop\nsingly connected\nno loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node.\nleaf\nA singly connected directed graph is also called a polytree. A singly connected undirected graph is\npolytree\ncalled a forest; if it is also connected, it is called a tree.\nforest\ntree\nWe can also define a notion of a forest, or of a tree, for directed graphs.\nDefinition 2.23\nA directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is\nalso connected.\nNote that polytrees are very di\u001berent from trees. For example, figure 2.5 shows a graph that is a\npolytree but is not a tree, because several nodes have more than one parent. As we will discuss\nlater in the book, loops in the graph increase the computational cost of various tasks.\nWe conclude this section with a final definition relating to loops in the graph. This definition\nwill play an important role in evaluating the cost of reasoning using graph-based representations.\nDefinition 2.24 Let X1\u2014X2\u2014 \u00b7 \u00b7 \u00b7 \u2014Xk\u2014X1 be a loop in the graph; a chord in the loop is an edge connecting\nchordal graph Xi and Xj for two nonconsecutive nodes Xi, Xj. An undirected graph H is said to be chordal if\nany loop X1\u2014X2\u2014 \u00b7 \u00b7 \u00b7 \u2014Xk\u2014X1 for k \u2265 4 has a chord.\nThus, for example, a loop A\u2014B\u2014C\u2014D\u2014A (as in figure 1.1b) is nonchordal, but adding an\nedge A\u2014C would render it chordal. In other words, in a chordal graph, the longest \u201cminimal\nloop\u201d (one that has no shortcut) is a triangle. Thus, chordal graphs are often also called\ntriangulated triangulated.\ngraph We can extend the notion of chordal graphs to graphs that contain directed edges.\nDefinition 2.25 A graph K is said to be chordal if its underlying undirected graph is chordal.", 
            "title": "4 Cycles and Loops"
        }, 
        {
            "location": "/representation_bayesian_network/", 
            "text": "Bayesian Network\n\n\nA Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.\n\n\nA Non-Causal Bayesian Network Example\n\n\nFigure 1 shows a simple Bayesian network, which consists of only two nodes and one link. It represents the JPD of the variables Eye Color and Hair Color in a population of students (Snee, 1974). In this case, the conditional probabilities of Hair Color given the values of its parent node, Eye Color, are provided in a CPT. It is important to point out that this Bayesian network does not contain any causal assumptions, i.e. we have no knowledge of the causal order between the variables. Thus, the interpretation of this network should be merely statistical (informational).\n\n\n\n\nA Causal Network Example\n\n\nFigure 2 illustrates another simple yet typical Bayesian network. In contrast to the statistical relationships in Figure 1, the diagram in Figure 2 describes the causal relationships among the seasons of the year (\nX_1\n), whether it is raining (\nX_2\n), whether the sprinkler is on (\nX_3\n), whether the pavement is wet (\nX_4\n), and whether the pavement is slippery (\nX_5\n). Here, the absence of a direct link between \nX_1\n and \nX_5\n, for example, captures our understanding that there is no direct influence of season on slipperiness. The influence is mediated by the wetness of the pavement (if freezing were a possibility, a direct link could be added). \n\n\n\n\nA Dynamic Bayesian Network Example\n\n\nEntities that live in a changing environment must keep track of variables whose values change over time. Dynamic Bayesian networks capture this process by representing multiple copies of the state variables, one for each time step. A set of variables \nX_{t-1}\n and \nX_t\n denotes the world state at times t-1 and t respectively. A set of evidence variables Et denotes the observations available at time t. The sensor model \nP(E_t|X_t)\n is encoded in the conditional probability distributions for the observable variables, given the state variables. The transition model \nP(X_t|X_{t-1})\n relates the state at time t-1 to the state at time t. Keeping track of the world means computing the current probability distribution over world states given all past observations, i.e. \nP(X_t|E_1,\u2026,E_t)\n.\n\n\nDynamic Bayesian networks (DBN) are a generalization of Hidden Markov Models (HMM) and Kalman Filters (KF). Every HMM and KF can be represented with a DBN. Furthermore, the DBN representation of an HMM is much more compact and, thus, much better understandable. The nodes in the HMM represent the states of the system, whereas the nodes in the DBN represent the dimensions of the system. For example, the HMM representation of the valve system in Figure 2.3 is made of 26 nodes and 36 arcs, versus 9 nodes and 11 arcs in the DBN (Weber and Jouffe, 2003).", 
            "title": "Bayesian Network"
        }, 
        {
            "location": "/representation_bayesian_network/#bayesian-network", 
            "text": "A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.", 
            "title": "Bayesian Network"
        }, 
        {
            "location": "/representation_bayesian_network/#a-non-causal-bayesian-network-example", 
            "text": "Figure 1 shows a simple Bayesian network, which consists of only two nodes and one link. It represents the JPD of the variables Eye Color and Hair Color in a population of students (Snee, 1974). In this case, the conditional probabilities of Hair Color given the values of its parent node, Eye Color, are provided in a CPT. It is important to point out that this Bayesian network does not contain any causal assumptions, i.e. we have no knowledge of the causal order between the variables. Thus, the interpretation of this network should be merely statistical (informational).", 
            "title": "A Non-Causal Bayesian Network Example"
        }, 
        {
            "location": "/representation_bayesian_network/#a-causal-network-example", 
            "text": "Figure 2 illustrates another simple yet typical Bayesian network. In contrast to the statistical relationships in Figure 1, the diagram in Figure 2 describes the causal relationships among the seasons of the year ( X_1 ), whether it is raining ( X_2 ), whether the sprinkler is on ( X_3 ), whether the pavement is wet ( X_4 ), and whether the pavement is slippery ( X_5 ). Here, the absence of a direct link between  X_1  and  X_5 , for example, captures our understanding that there is no direct influence of season on slipperiness. The influence is mediated by the wetness of the pavement (if freezing were a possibility, a direct link could be added).", 
            "title": "A Causal Network Example"
        }, 
        {
            "location": "/representation_bayesian_network/#a-dynamic-bayesian-network-example", 
            "text": "Entities that live in a changing environment must keep track of variables whose values change over time. Dynamic Bayesian networks capture this process by representing multiple copies of the state variables, one for each time step. A set of variables  X_{t-1}  and  X_t  denotes the world state at times t-1 and t respectively. A set of evidence variables Et denotes the observations available at time t. The sensor model  P(E_t|X_t)  is encoded in the conditional probability distributions for the observable variables, given the state variables. The transition model  P(X_t|X_{t-1})  relates the state at time t-1 to the state at time t. Keeping track of the world means computing the current probability distribution over world states given all past observations, i.e.  P(X_t|E_1,\u2026,E_t) .  Dynamic Bayesian networks (DBN) are a generalization of Hidden Markov Models (HMM) and Kalman Filters (KF). Every HMM and KF can be represented with a DBN. Furthermore, the DBN representation of an HMM is much more compact and, thus, much better understandable. The nodes in the HMM represent the states of the system, whereas the nodes in the DBN represent the dimensions of the system. For example, the HMM representation of the valve system in Figure 2.3 is made of 26 nodes and 36 arcs, versus 9 nodes and 11 arcs in the DBN (Weber and Jouffe, 2003).", 
            "title": "A Dynamic Bayesian Network Example"
        }, 
        {
            "location": "/representation_template_models/", 
            "text": "Template Models for Bayesian Networks\n\n\nIn many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models.\n\n\nTemporal Models\n\n\nOur focus in this section is on modeling dynamic settings, where we are interested in reasoning about the state of the world as it evolves over time. We can model such settings in terms of a system state system state, whose value at time t is a snapshot of the relevant attributes (hidden or observed) of the system at time t. We assume that the system state is represented, as usual, as an assignment of values to some set of random variables X . We use X (t) i to represent the instantiation of the variable Xi at time t. Note that Xi itself is no longer a variable that takes a value; rather, it is a template variable template variable. This template is instantiated at di\u001berent points in time t, and each Xi (t) is a variable that takes a value in Val(Xi). For a set of variables X \u2286 X , we use X (t1:t2) (t1 \n t2) to denote the set of variables {X (t) : t \u2208 [t1,t2]}. As usual, we use the notation x(t:t0) for an\nassignment of values to this set of variables.\n\n\nEach \u201cpossible world\u201d in our probability space is now a trajectory: an assignment of values to each variable X (t) i for each relevant time t. Our goal therefore is to represent a joint distribution over such trajectories. Clearly, the space of possible trajectories is a very complex probability space, so representing such a distribution can be very di\u001ecult. We therefore make a series of simplifying assumptions that help make this representational problem more tractable.\n\n\nDynamic Bayesian Networks\n\n\n\n\n\n\nDirected Probabilistic Models for Object-Relational Domains\n\n\nBased on the framework described in the previous section, we now describe template-based representation languages that can encode directed probabilistic models.\n\n\nPlate Models\n\n\nWe begin our discussion by presenting the \nplate model\n, the simplest and best-established of the object-relational frameworks. Although restricted in several important ways, the plate modeling framework is perhaps the approach that has been most commonly used in practice, notably for encoding the assumptions made in various learning tasks. This framework also provides an excellent starting point for describing the key ideas of template-based languages and for motivating some of the extensions that have been pursued in richer languages.\n\n\nIn the plate formalism, object types are called \nplates\n. The fact that multiple objects in the class share the same set of attributes and same probabilistic model is the basis for the use of the term \u201cplate,\u201d which suggests a stack of identical objects. We begin with some motivating examples and then describe the formal framework.\n\n\nExamples\n\n\nExample 1\n \nThe simplest example of a plate model, shown in figure 6.6, describes multiple random variables generated from the same distribution. In this case, we have a set of random variables \n X(d)\\ (d \u2208 D) \n that all have the same domain Val(X) and are sampled from the same distribution. In a plate representation, we encode the fact that these variables are all generated from the same template by drawing only a single node X(d) and enclosing it in a box denoting that d ranges over D, so that we know that the box represents an entire \u201cstack\u201d of these identically distributed variables. This box plate is called a plate, with the analogy that it represents a stack of identical plates.", 
            "title": "Template Models for Bayesian Networks"
        }, 
        {
            "location": "/representation_template_models/#template-models-for-bayesian-networks", 
            "text": "In many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models.", 
            "title": "Template Models for Bayesian Networks"
        }, 
        {
            "location": "/representation_template_models/#temporal-models", 
            "text": "Our focus in this section is on modeling dynamic settings, where we are interested in reasoning about the state of the world as it evolves over time. We can model such settings in terms of a system state system state, whose value at time t is a snapshot of the relevant attributes (hidden or observed) of the system at time t. We assume that the system state is represented, as usual, as an assignment of values to some set of random variables X . We use X (t) i to represent the instantiation of the variable Xi at time t. Note that Xi itself is no longer a variable that takes a value; rather, it is a template variable template variable. This template is instantiated at di\u001berent points in time t, and each Xi (t) is a variable that takes a value in Val(Xi). For a set of variables X \u2286 X , we use X (t1:t2) (t1   t2) to denote the set of variables {X (t) : t \u2208 [t1,t2]}. As usual, we use the notation x(t:t0) for an\nassignment of values to this set of variables.  Each \u201cpossible world\u201d in our probability space is now a trajectory: an assignment of values to each variable X (t) i for each relevant time t. Our goal therefore is to represent a joint distribution over such trajectories. Clearly, the space of possible trajectories is a very complex probability space, so representing such a distribution can be very di\u001ecult. We therefore make a series of simplifying assumptions that help make this representational problem more tractable.", 
            "title": "Temporal Models"
        }, 
        {
            "location": "/representation_template_models/#dynamic-bayesian-networks", 
            "text": "", 
            "title": "Dynamic Bayesian Networks"
        }, 
        {
            "location": "/representation_template_models/#directed-probabilistic-models-for-object-relational-domains", 
            "text": "Based on the framework described in the previous section, we now describe template-based representation languages that can encode directed probabilistic models.", 
            "title": "Directed Probabilistic Models for Object-Relational Domains"
        }, 
        {
            "location": "/representation_template_models/#plate-models", 
            "text": "We begin our discussion by presenting the  plate model , the simplest and best-established of the object-relational frameworks. Although restricted in several important ways, the plate modeling framework is perhaps the approach that has been most commonly used in practice, notably for encoding the assumptions made in various learning tasks. This framework also provides an excellent starting point for describing the key ideas of template-based languages and for motivating some of the extensions that have been pursued in richer languages.  In the plate formalism, object types are called  plates . The fact that multiple objects in the class share the same set of attributes and same probabilistic model is the basis for the use of the term \u201cplate,\u201d which suggests a stack of identical objects. We begin with some motivating examples and then describe the formal framework.", 
            "title": "Plate Models"
        }, 
        {
            "location": "/representation_template_models/#examples", 
            "text": "Example 1   The simplest example of a plate model, shown in figure 6.6, describes multiple random variables generated from the same distribution. In this case, we have a set of random variables   X(d)\\ (d \u2208 D)   that all have the same domain Val(X) and are sampled from the same distribution. In a plate representation, we encode the fact that these variables are all generated from the same template by drawing only a single node X(d) and enclosing it in a box denoting that d ranges over D, so that we know that the box represents an entire \u201cstack\u201d of these identically distributed variables. This box plate is called a plate, with the analogy that it represents a stack of identical plates.", 
            "title": "Examples"
        }, 
        {
            "location": "/factor_graph/", 
            "text": "Factor Graph\n\n\nA factor graph is a bipartite graph representing the factorization of a function.\n\n\nEach edge in graph defines a function\n\n\nDefinition\n\n\nA factor graph is a bipartite graph representing the factorization of a function.\n\n\nRelated Readings\n\n\n[1]: Factor Graph, \nwikipedia.org", 
            "title": "Factor Graph"
        }, 
        {
            "location": "/factor_graph/#factor-graph", 
            "text": "A factor graph is a bipartite graph representing the factorization of a function.  Each edge in graph defines a function", 
            "title": "Factor Graph"
        }, 
        {
            "location": "/factor_graph/#definition", 
            "text": "A factor graph is a bipartite graph representing the factorization of a function.", 
            "title": "Definition"
        }, 
        {
            "location": "/factor_graph/#related-readings", 
            "text": "[1]: Factor Graph,  wikipedia.org", 
            "title": "Related Readings"
        }, 
        {
            "location": "/inference/", 
            "text": "Inference\n\n\n\n\nThis addresses the question of probabilistic inference: how a PGM can be used to answer questions.\n\n\n\n\nEven though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem.", 
            "title": "Inference"
        }, 
        {
            "location": "/inference/#inference", 
            "text": "This addresses the question of probabilistic inference: how a PGM can be used to answer questions.   Even though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem.", 
            "title": "Inference"
        }, 
        {
            "location": "/learning/", 
            "text": "Learning\n\n\n\n\nThis course addresses the question of learning: how a PGM can be learned from a data set of examples.\n\n\n\n\nThe course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem.", 
            "title": "Learning"
        }, 
        {
            "location": "/learning/#learning", 
            "text": "This course addresses the question of learning: how a PGM can be learned from a data set of examples.   The course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem.", 
            "title": "Learning"
        }, 
        {
            "location": "/software_unbbayes/", 
            "text": "An Introduction to UnBBayes\n\n\nUnBBayes is a probabilistic network framework written in Java. It has both a GUI and an API with inference, sampling, learning and evaluation. It supports Bayesian networks, influence diagrams, MSBN, OOBN, HBN, MEBN/PR-OWL, PRM, structure, parameter and incremental learning.\n\n\nFeatures\n\n\n\n\nProbabilistic Networks:\n\n\nBayesian Network (BN)\n\n\nJunction Tree\n\n\nLikelihood Weighting\n\n\nGibbs\n\n\n\n\n\n\nInfluence Diagram (ID)\n\n\nMultiply Sectioned Bayesian Network (MSBN)\n\n\nHybrid Bayesian Network (HBN)\n\n\nGaussian Mixture - Propagation under development\n\n\n\n\n\n\nObject-Oriented Bayesian Network (OOBN)\n\n\n\n\n\n\nFOL Probabilistic Network:\n\n\nMulti-Entity Bayesian Network (MEBN)\n\n\nProbabilistic Ontology Language (PR-OWL)\n\n\n\n\n\n\nLearning Bayesian Network:\n\n\nK2\n\n\nB\n\n\nCBL-A\n\n\nCBL-B\n\n\nIncremental Learning\n\n\n\n\n\n\nSampling\n\n\nLogic\n\n\nLikelihood Weighting\n\n\nGibbs\n\n\n\n\n\n\nClassification Performance Evaluation\n\n\nEvaluation using Logic Sampling\n\n\nEvaluation using Likelihood Weighting Sampling\n\n\n\n\n\n\n\n\nInstallation\n\n\n\n\nGo to \nhttps://sourceforge.net/projects/unbbayes/files/latest/download?source=typ_redirect\n to download zip file\n\n\nExtract file \nunbbayes-4.21.18.zip\n  to \nunbbayes-4.21.18\n folder\n\n\nOpen \nunbbayes-4.21.18\n folder, double click to \nunbbayes.bat\n\n\n\n\nunbbayes-4.21.18\n open\n\n\n\nOfficial Videos\n\n\nIn this section, I add some official videos from unbbayes team. There are overview\n\n\nOverview\n\n\nIn this video we are going to show the basic function we have in UnBBayes. This is the first of many tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.\n\n\n\n\n\n\n\n\nBayesian Network\n\n\nIn this video we are going to show how to create and compile a Bayesian Network (BN) in UnBBayes. This is our second of many video tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.\n\n\n\n\n\n\n\n\nUnBBayes Performance Evaluation for Multi-Sensor Classification Systems\n\n\nIn this video we are going to show how to do a performance evaluation for multi-sensor classification systems in UnBBayes. It has been a while we do not post new videos, but hopefully this third one is just one more of many tutorials we will have available to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.\n\n\n\n\n\n\n\n\nProbabilistic Ontology Modeling Using UnBBayes\n\n\nIn this video we discuss how to model probabilistic ontologies using PR-OWL/MEBN in UnBBayes. This session was a video conference between PhD students from the Institute of Business Administration (http://www.iba.edu.pk) and Rommel Carvalho from George Mason University (http://www.gmu.edu).", 
            "title": "An Introduction to UnBBayes"
        }, 
        {
            "location": "/software_unbbayes/#an-introduction-to-unbbayes", 
            "text": "UnBBayes is a probabilistic network framework written in Java. It has both a GUI and an API with inference, sampling, learning and evaluation. It supports Bayesian networks, influence diagrams, MSBN, OOBN, HBN, MEBN/PR-OWL, PRM, structure, parameter and incremental learning.", 
            "title": "An Introduction to UnBBayes"
        }, 
        {
            "location": "/software_unbbayes/#features", 
            "text": "Probabilistic Networks:  Bayesian Network (BN)  Junction Tree  Likelihood Weighting  Gibbs    Influence Diagram (ID)  Multiply Sectioned Bayesian Network (MSBN)  Hybrid Bayesian Network (HBN)  Gaussian Mixture - Propagation under development    Object-Oriented Bayesian Network (OOBN)    FOL Probabilistic Network:  Multi-Entity Bayesian Network (MEBN)  Probabilistic Ontology Language (PR-OWL)    Learning Bayesian Network:  K2  B  CBL-A  CBL-B  Incremental Learning    Sampling  Logic  Likelihood Weighting  Gibbs    Classification Performance Evaluation  Evaluation using Logic Sampling  Evaluation using Likelihood Weighting Sampling", 
            "title": "Features"
        }, 
        {
            "location": "/software_unbbayes/#installation", 
            "text": "Go to  https://sourceforge.net/projects/unbbayes/files/latest/download?source=typ_redirect  to download zip file  Extract file  unbbayes-4.21.18.zip   to  unbbayes-4.21.18  folder  Open  unbbayes-4.21.18  folder, double click to  unbbayes.bat   unbbayes-4.21.18  open", 
            "title": "Installation"
        }, 
        {
            "location": "/software_unbbayes/#official-videos", 
            "text": "In this section, I add some official videos from unbbayes team. There are overview", 
            "title": "Official Videos"
        }, 
        {
            "location": "/software_unbbayes/#overview", 
            "text": "In this video we are going to show the basic function we have in UnBBayes. This is the first of many tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.", 
            "title": "Overview"
        }, 
        {
            "location": "/software_unbbayes/#bayesian-network", 
            "text": "In this video we are going to show how to create and compile a Bayesian Network (BN) in UnBBayes. This is our second of many video tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.", 
            "title": "Bayesian Network"
        }, 
        {
            "location": "/software_unbbayes/#unbbayes-performance-evaluation-for-multi-sensor-classification-systems", 
            "text": "In this video we are going to show how to do a performance evaluation for multi-sensor classification systems in UnBBayes. It has been a while we do not post new videos, but hopefully this third one is just one more of many tutorials we will have available to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.", 
            "title": "UnBBayes Performance Evaluation for Multi-Sensor Classification Systems"
        }, 
        {
            "location": "/software_unbbayes/#probabilistic-ontology-modeling-using-unbbayes", 
            "text": "In this video we discuss how to model probabilistic ontologies using PR-OWL/MEBN in UnBBayes. This session was a video conference between PhD students from the Institute of Business Administration (http://www.iba.edu.pk) and Rommel Carvalho from George Mason University (http://www.gmu.edu).", 
            "title": "Probabilistic Ontology Modeling Using UnBBayes"
        }, 
        {
            "location": "/data_medical_domain/", 
            "text": "Medical Domain Data\n\n\nWe have provided you with a joint probability distribution of symptons, conditions and diseases based on the \"flu\" example in class. Certain diseases are more likely than others given certain symptons, and a model such as this can be used to help doctors make a diagnosis.  (Don't actually use this for diagnosis, though!). The ground-truth joint probability distribution consists of twelve binary random variables and contains \n 2^{12} \n possible configurations (numbered 0 to 4095), which is small enough that you can enumerate them exhaustively. The variables are as follows:\n\n\n\n\n(0) \nIsSummer\n \ntrue\n if it is the summer season, false otherwise.\n\n\n(1) \nHasFlu\n \ntrue\n if the patient has the flu.\n\n\n(2) \nHasFoodPoisoning\n \ntrue\n if the patient has food poisoning.\n\n\n(3) \nHasHayFever\n \ntrue\n if patient has hay fever.\n\n\n(4) \nHasPneumonia\n \ntrue\n if the patient has pneumonia.\n\n\n(5) \nHasRespiratoryProblems\n \ntrue\n if the patient has problems in the respiratory system.\n\n\n(6) \nHasGastricProblems\n \ntrue\n if the patient has problems in the gastro-intestinal system.\n\n\n(7) \nHasRash\n \ntrue\n if the patient has a skin rash.\n\n\n(8) \nCoughs\n \ntrue\n if the patient has a cough.\n\n\n(9) \nIsFatigued\n \ntrue\n if the patient is tired and fatigued.\n\n\n(10) \nVomits\n \ntrue\n if the patient has vomited.\n\n\n(11) \nHasFever\n \ntrue\n if the patient has a high fever.\n\n\n\n\nYou can download all the data \nhere\n. The archive contains two files:\n\n\n\n\njoint.dat\n: The true joint probability distribution over the twelve binary variables. Since each variable is binary, we can represent a * full variable assignment as a bitstring. This file lists all 2^12 assignments (one in each line) as pairs \"Integer Probability\" where \"Integer\" is an integer encoding of the bitstring. Specifically, assuming false=0 and true=1, an assignment to all variables results in a 12-bit binary number (with the index of the variables shown in parantheses above) which is converted to a decimal number. For example, assignment 0 represents all variables are false, 1 represents only IsSummer is true, 2 represents only HasFlu is true, and so on.\n\n\ndataset.dat\n: The dataset consists of samples from the above probability distribution. Each line of the file contains a complete assignment to all the variables, encoded as an integer (as described above).", 
            "title": "Medical Domain Data"
        }, 
        {
            "location": "/data_medical_domain/#medical-domain-data", 
            "text": "We have provided you with a joint probability distribution of symptons, conditions and diseases based on the \"flu\" example in class. Certain diseases are more likely than others given certain symptons, and a model such as this can be used to help doctors make a diagnosis.  (Don't actually use this for diagnosis, though!). The ground-truth joint probability distribution consists of twelve binary random variables and contains   2^{12}   possible configurations (numbered 0 to 4095), which is small enough that you can enumerate them exhaustively. The variables are as follows:   (0)  IsSummer   true  if it is the summer season, false otherwise.  (1)  HasFlu   true  if the patient has the flu.  (2)  HasFoodPoisoning   true  if the patient has food poisoning.  (3)  HasHayFever   true  if patient has hay fever.  (4)  HasPneumonia   true  if the patient has pneumonia.  (5)  HasRespiratoryProblems   true  if the patient has problems in the respiratory system.  (6)  HasGastricProblems   true  if the patient has problems in the gastro-intestinal system.  (7)  HasRash   true  if the patient has a skin rash.  (8)  Coughs   true  if the patient has a cough.  (9)  IsFatigued   true  if the patient is tired and fatigued.  (10)  Vomits   true  if the patient has vomited.  (11)  HasFever   true  if the patient has a high fever.   You can download all the data  here . The archive contains two files:   joint.dat : The true joint probability distribution over the twelve binary variables. Since each variable is binary, we can represent a * full variable assignment as a bitstring. This file lists all 2^12 assignments (one in each line) as pairs \"Integer Probability\" where \"Integer\" is an integer encoding of the bitstring. Specifically, assuming false=0 and true=1, an assignment to all variables results in a 12-bit binary number (with the index of the variables shown in parantheses above) which is converted to a decimal number. For example, assignment 0 represents all variables are false, 1 represents only IsSummer is true, 2 represents only HasFlu is true, and so on.  dataset.dat : The dataset consists of samples from the above probability distribution. Each line of the file contains a complete assignment to all the variables, encoded as an integer (as described above).", 
            "title": "Medical Domain Data"
        }, 
        {
            "location": "/data_owc/", 
            "text": "Optical Word Recognition\n\n\nWe will be studying the computer vision task of recognizing words from images. The task of recognizing words is usually decomposed to recognition of individual characters from their respective images (optical character recognition, OCR), and hence inferring the word. However character recognition is often a very difficult task, and since each character is predicted independent of its neighbors, its results can often contain combinations of characters that may not be possible in English. In this homework we will augment a simple OCR model with additional factors that capture some intuitions based on character co-occurences and image similarities.\n\n\n\n\nThe undirected graphical model for recognition of a given word is given in the figure above. It consists of two types of variables:\n\n\n\n\nImage Variables\n: These are observed images that we need to predict the corresponsing character of, and the number of these image variables for a word is the number of characters in the word. The value of these image variables is an observed image, represented by an integer id (less than 1000). For the description of the model, assume the id of the image at position i is represented by img(i).\n\n\nCharacter Variables\n: These are unobserved variables that represent the character prediction for each of the images, and there is one of these for each of the image variables. For our dataset, the domain of these variables is restricted to the ten most frequent characters in the English language ({e,t,a,o,i,n,s,h,r,d} \n[ciation]\n), instead of the complete alphabet. For the discussion below, assume the predicted character at position i is represented by char(i).\n\n\n\n\nThe model for a word w will consist of len(w) observed image ids, and the same number of unobserved character variables. For a given assignment to these character variables, the model score will be specified using three types of factors:\n\n\n\n\nOCR Factors, \n \\psi_{o} \n : These factors capture the predictions of a character-based OCR system, and hence exist between every image variable and its corresponding character variable. The number of these factors of word w is \nlen(w)\n. The value of factor between an image variable and the character variable at position i is dependent on \nimg(i)\n and \nchar(i)\n, and is stored in \nocr.dat\n file described in the data section.\n\n\nTransition Factors, \n \\psi_{t} \n : Since we also want to represent the co-occurence frequencies of the characters in our model, we add these factors between all consecutive character variables. The number of these factors of word \nw\n is \nlen(w)-1\n. The value of factor between two character variables at positions \ni\n and \ni+1\n is dependent on \nchar(i)\n and \nchar(i+1)\n, and is high if \nchar(i+1)\n is frequently preceded by \nchar(i)\n in english words. These values are given to you in \ntrans.dat\n file described in the data section.\n\n\nSkip Factors, \n \\psi_{s} \n : Another intuition that we would like to capture in our model is that similar images in a word always represent the same character. Thus our model score should be higher if it predicts the same characters for similar images. These factors exist between every pair of image variables that have the same id, i.e. this factor exist between all \ni\n,\nj\n, \ni!=j\n such that \nimg(i)==img(j)\n. The value of this factor depends on \nchar(i)\n and \nchar(j)\n, and is 5.0 if \nchar(i)==char(j)\n, and 1.0 otherwise.\n\n\n\n\nYou can download all the data \nhere\n. The archive contains the following files:\n\n\n\n\nocr.dat\n: Contains the output predictions of a pre-existing OCR system for the set of thousand images. Each row contains three tab separated values \"id a prob\" and represents the OCR system's probability that image id represents character \n a \n, \n p(char=a | img=id) = prob \n. Use these values directly as the value of the factor between image and character variables at position \n  i \n, \n \\psi_o (image(i)=id, char(i)=a) = prob \n. Since there are 10 characters and 1000 images, the total number of rows in this file is 10,000.\n\n\ntrans.dat\n: Stores the factor potentials for the transition factors. Each row contains three tab-separated values \"a b value\" that represents the value of factor when the previous character is \"a\" and the next character is \"b\", i.e. (char(i)=a, char(i+1)=b) = value. The number of rows in the file is 100 (10*10).\ndata.dat (and truth.dat): Dataset to run your experiments on (see Core Tasks below). The observed dataset (data.dat) consists observed images of one word on each row. The observed images for a word are represented by a sequence of tab-separated integer ids (\"id1 id2 id3\"). The true word for these observed set of images is stored the respective row in truth.dat, and is simply a string (\"eat\"). For the core task (3) below, you should iterate through both the files together to ensure you have the true word along with the observed images.\n\n\nExtra files (\nbicounts.dat\n, \nallwords.dat\n, \nallimagesX.dat\n): These files are not necessary for the core tasks, but may be useful for further fun and your own exploration. allwords.dat and allimagesX.dat are larger versions of data.dat and truth.dat, i.e. they contain all possible words that can be generated from our restricted set of alphabet, and five samples of their observed image sequences (one in each file). You can run inference on these if you like, but is likely to take 15-20 times longer than the small dataset. bicount.dat is in the same format as trans.dat, but instead of storing inexplicable potentials, it stores the joint probability of the co-occurences of the characters.\n\n\n\n\nCore Task\n\n\n1.\n \nGraphical Model\n: Implement the graphical model containing the factors above. For any given assignment to the character variables, your model should be able to calculate the model score. Implemention should allow switching between three models:\n\n\n\n\nOCR model: only contains the OCR factors\n\n\nTransition model: contains OCR and Transition factors\n\n\nCombined model: containing all three types of factors\n\n\n\n\nNote\n: To avoid errors arising from numerical issues, we suggest you represent the factors in the log-space and take sums as much as possible, calculating the log of the model score.\n\n\n2. Exhaustive Inference\n: Using the graphical model, write code to perform exhaustive inference, i.e. your code should be able to calculate the probability of any assignment of the character and image variables. To calculate the normalization constant Z for the word w, you will need to go through all possible assignments to the character variables (there will be \n 10^{len(w)} \n of these).\n\n\n3. Model Accuracy:\n Run your model on the data given in the file data.dat. For every word in the dataset, pick the assignment to character variables that has the highest probability according to the model, and treat this as the model prediction for the word. Using the truth given in truth.dat, compare the accuracy of the model predictions using the following three metrics:\n1. Character-wise accuracy: Ratio of correctly predicted characters to total number of characters\n2. Word-wise accuracy: Ratio of correctly predicted words to total number of words\n3. Average Dataset log-likelihood: For each word given in data.dat, calculate the log of the probability of the true word according to the model. Compute the average of this value for the whole dataset.\n\n\nCompare all of the three models described in (1) using these three metrics. Also give some examples of words that were incorrect by the OCR model but consequently fixed by the Transition model, and examples of words that were incorrect by the OCR, partially corrected by the Transition model, and then completely fixed by the Combined model.", 
            "title": "Optical Word Recognition Dataset"
        }, 
        {
            "location": "/data_owc/#optical-word-recognition", 
            "text": "We will be studying the computer vision task of recognizing words from images. The task of recognizing words is usually decomposed to recognition of individual characters from their respective images (optical character recognition, OCR), and hence inferring the word. However character recognition is often a very difficult task, and since each character is predicted independent of its neighbors, its results can often contain combinations of characters that may not be possible in English. In this homework we will augment a simple OCR model with additional factors that capture some intuitions based on character co-occurences and image similarities.   The undirected graphical model for recognition of a given word is given in the figure above. It consists of two types of variables:   Image Variables : These are observed images that we need to predict the corresponsing character of, and the number of these image variables for a word is the number of characters in the word. The value of these image variables is an observed image, represented by an integer id (less than 1000). For the description of the model, assume the id of the image at position i is represented by img(i).  Character Variables : These are unobserved variables that represent the character prediction for each of the images, and there is one of these for each of the image variables. For our dataset, the domain of these variables is restricted to the ten most frequent characters in the English language ({e,t,a,o,i,n,s,h,r,d}  [ciation] ), instead of the complete alphabet. For the discussion below, assume the predicted character at position i is represented by char(i).   The model for a word w will consist of len(w) observed image ids, and the same number of unobserved character variables. For a given assignment to these character variables, the model score will be specified using three types of factors:   OCR Factors,   \\psi_{o}   : These factors capture the predictions of a character-based OCR system, and hence exist between every image variable and its corresponding character variable. The number of these factors of word w is  len(w) . The value of factor between an image variable and the character variable at position i is dependent on  img(i)  and  char(i) , and is stored in  ocr.dat  file described in the data section.  Transition Factors,   \\psi_{t}   : Since we also want to represent the co-occurence frequencies of the characters in our model, we add these factors between all consecutive character variables. The number of these factors of word  w  is  len(w)-1 . The value of factor between two character variables at positions  i  and  i+1  is dependent on  char(i)  and  char(i+1) , and is high if  char(i+1)  is frequently preceded by  char(i)  in english words. These values are given to you in  trans.dat  file described in the data section.  Skip Factors,   \\psi_{s}   : Another intuition that we would like to capture in our model is that similar images in a word always represent the same character. Thus our model score should be higher if it predicts the same characters for similar images. These factors exist between every pair of image variables that have the same id, i.e. this factor exist between all  i , j ,  i!=j  such that  img(i)==img(j) . The value of this factor depends on  char(i)  and  char(j) , and is 5.0 if  char(i)==char(j) , and 1.0 otherwise.   You can download all the data  here . The archive contains the following files:   ocr.dat : Contains the output predictions of a pre-existing OCR system for the set of thousand images. Each row contains three tab separated values \"id a prob\" and represents the OCR system's probability that image id represents character   a  ,   p(char=a | img=id) = prob  . Use these values directly as the value of the factor between image and character variables at position    i  ,   \\psi_o (image(i)=id, char(i)=a) = prob  . Since there are 10 characters and 1000 images, the total number of rows in this file is 10,000.  trans.dat : Stores the factor potentials for the transition factors. Each row contains three tab-separated values \"a b value\" that represents the value of factor when the previous character is \"a\" and the next character is \"b\", i.e. (char(i)=a, char(i+1)=b) = value. The number of rows in the file is 100 (10*10).\ndata.dat (and truth.dat): Dataset to run your experiments on (see Core Tasks below). The observed dataset (data.dat) consists observed images of one word on each row. The observed images for a word are represented by a sequence of tab-separated integer ids (\"id1 id2 id3\"). The true word for these observed set of images is stored the respective row in truth.dat, and is simply a string (\"eat\"). For the core task (3) below, you should iterate through both the files together to ensure you have the true word along with the observed images.  Extra files ( bicounts.dat ,  allwords.dat ,  allimagesX.dat ): These files are not necessary for the core tasks, but may be useful for further fun and your own exploration. allwords.dat and allimagesX.dat are larger versions of data.dat and truth.dat, i.e. they contain all possible words that can be generated from our restricted set of alphabet, and five samples of their observed image sequences (one in each file). You can run inference on these if you like, but is likely to take 15-20 times longer than the small dataset. bicount.dat is in the same format as trans.dat, but instead of storing inexplicable potentials, it stores the joint probability of the co-occurences of the characters.", 
            "title": "Optical Word Recognition"
        }, 
        {
            "location": "/data_owc/#core-task", 
            "text": "1.   Graphical Model : Implement the graphical model containing the factors above. For any given assignment to the character variables, your model should be able to calculate the model score. Implemention should allow switching between three models:   OCR model: only contains the OCR factors  Transition model: contains OCR and Transition factors  Combined model: containing all three types of factors   Note : To avoid errors arising from numerical issues, we suggest you represent the factors in the log-space and take sums as much as possible, calculating the log of the model score.  2. Exhaustive Inference : Using the graphical model, write code to perform exhaustive inference, i.e. your code should be able to calculate the probability of any assignment of the character and image variables. To calculate the normalization constant Z for the word w, you will need to go through all possible assignments to the character variables (there will be   10^{len(w)}   of these).  3. Model Accuracy:  Run your model on the data given in the file data.dat. For every word in the dataset, pick the assignment to character variables that has the highest probability according to the model, and treat this as the model prediction for the word. Using the truth given in truth.dat, compare the accuracy of the model predictions using the following three metrics:\n1. Character-wise accuracy: Ratio of correctly predicted characters to total number of characters\n2. Word-wise accuracy: Ratio of correctly predicted words to total number of words\n3. Average Dataset log-likelihood: For each word given in data.dat, calculate the log of the probability of the true word according to the model. Compute the average of this value for the whole dataset.  Compare all of the three models described in (1) using these three metrics. Also give some examples of words that were incorrect by the OCR model but consequently fixed by the Transition model, and examples of words that were incorrect by the OCR, partially corrected by the Transition model, and then completely fixed by the Combined model.", 
            "title": "Core Task"
        }
    ]
}