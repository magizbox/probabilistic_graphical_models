{
    "docs": [
        {
            "location": "/", 
            "text": "Natural Language Processing\n\n\n\n\nNatural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.\n\n\nNLP is related to the area of human\u2013computer interaction. Many challenges in NLP involve: natural language understanding, enabling computers to derive meaning from human or natural language input; and others involve natural language generation.\n\n\nThe input and output of an NLP system can be either speech or written text.\n\n\n\n\nComponents of NLP\n\n\nThere are two components of NLP as given\n\n\n\n\nNatural Language Understanding (NLU)\n: this task mapping the given input in natural language into useful representations and analyzing different aspects of the language.\n\n\nNatural Language Generation (NLG)\n: In the process of producing meaningful phrases and sentences in the form of natural language form some internal representation. It involves \ntext planning\n retrieve the relevant content from knowledge base, \nsentence planning\n choose required words, forming meaningful phrases, setting tone of the sentence, \ntext realization\n map sentence plan into sentence structure.\n\n\n\n\nDifficulties\n\n\nNatural Language has an extremely rich form and structure. It is very ambiguous. There can be different levels of ambiguity\n\n\n\n\nLexical ambiguity\n: it is at very primitive level such as word-level. For example, treating the word \"board\" as noun or verb?\n\n\nSyntax level ambiguity\n: A sentence be parsed in different ways. For example, \"He lifted the beetle with the red cap?\" - did he use cap to lift the beetle or he lifted a beetle that had red cap?\n\n\nReferential ambiguity\n: referring to something using pronouns. For example, Rima went to Gauri. She said \"I am tired\". - Exactly who is tired?\n\n\nOne input can mean different meanings.\n\n\nMany inputs can mean the same thing.\n\n\n\n\nBooks\n\n\n\n\n\n\n\n\nCourses", 
            "title": "Home"
        }, 
        {
            "location": "/#natural-language-processing", 
            "text": "Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.  NLP is related to the area of human\u2013computer interaction. Many challenges in NLP involve: natural language understanding, enabling computers to derive meaning from human or natural language input; and others involve natural language generation.  The input and output of an NLP system can be either speech or written text.   Components of NLP  There are two components of NLP as given   Natural Language Understanding (NLU) : this task mapping the given input in natural language into useful representations and analyzing different aspects of the language.  Natural Language Generation (NLG) : In the process of producing meaningful phrases and sentences in the form of natural language form some internal representation. It involves  text planning  retrieve the relevant content from knowledge base,  sentence planning  choose required words, forming meaningful phrases, setting tone of the sentence,  text realization  map sentence plan into sentence structure.   Difficulties  Natural Language has an extremely rich form and structure. It is very ambiguous. There can be different levels of ambiguity   Lexical ambiguity : it is at very primitive level such as word-level. For example, treating the word \"board\" as noun or verb?  Syntax level ambiguity : A sentence be parsed in different ways. For example, \"He lifted the beetle with the red cap?\" - did he use cap to lift the beetle or he lifted a beetle that had red cap?  Referential ambiguity : referring to something using pronouns. For example, Rima went to Gauri. She said \"I am tired\". - Exactly who is tired?  One input can mean different meanings.  Many inputs can mean the same thing.", 
            "title": "Natural Language Processing"
        }, 
        {
            "location": "/#books", 
            "text": "", 
            "title": "Books"
        }, 
        {
            "location": "/#courses", 
            "text": "", 
            "title": "Courses"
        }, 
        {
            "location": "/task/", 
            "text": "NLP Tasks\n\n\nThe analysis of natural language is broken into various board levels such as phonological, morphological, syntactic, semantic, pragmatic and discourse analysis.\n\n\n\n\nPhonological Analysis\n\n\nPhonology is analysis of spoken language. Therefore, it deals with speech recognition and generation. The core task of speech recognition and generation system is to take an acoustic waveform as input and produce as output, a string of words. The phonology is a part of natural language analysis, which deals with it. The area of computational linguistics that deals with speech analysis is computational phonology\n\n\nExample:\n \nHans Rosling's shortest TED talk\n\n\n\n\n\n\n\nOriginal Sound\n\n\n\n\n\n  \n\nYour browser does not support the audio element.\n\n\n\n\n\n\n\n\n\nText\n\n\n\nX means unknown but the world is pretty known it's seven billion people have seven stones. One billion can save money to fly abroad on holiday every year. One billion can save money to keep a car or buy a car. And then three billion they save money to pay the by be a bicycle or perhaps a two-wheeler. And two billion they are busy saving money to buy shoes. In the future they will get rich and these people we move over here, these people will move over here, we will have two billion more in the world like this and the question is whether the rich people over there are prepared to be integrated in the world with 10 bilions people.\n\n\n\n\n\n\n\nAuto generated sound\n\n\n\n\n\n  \n\nYour browser does not support the audio element.\n\n\n\n\n\n\n\n\n\n\nMorphological Analysis\n\n\nIt is the most elementary phase of NLP. It deals with the word formation. In this phase, individual words are analyzed according to their components called \n\"morphemes\"\n. In addition, non-word taken such as punctuation, etc. are separated from words. Morpheme is basic grammatical building block that makes words.\n\n\n\n\nThe study of word structure is refereed to as morphology. In natural language processing, it is done in morphological analysis. The task of breaking a word into its morphemes is called morphological parsing. A morpheme is defined as minimal meaningful unit in a language, which cannot be further broken into smaller units.\n\n\nExample: word \nfox\n consists a single morpheme, as it cannot be further resolved into smaller units. Whereas word \ncats\n consists two morphemes, the morpheme \"cat\" and morpheme \"s\" indicating plurality.\n\n\nHere we defined the term meaningful. Though cat can be broken in \"c\" and \"at\", but these do not relate with word \"cat\" in any sense. Thus word \"cat\" will be dealt with as minimum meaningful unit.\n\n\nMorphemes are traditionally divided into two types\n\n\n\n\n(i) \"free morphemes\", that are able to act as words in isolation (e.g., \"thing\", \"permanent\", \"local\")\n\n\n(ii) \"bound morphemes\", that can operate only as part of other words (e.g., \"is\" 'ing' etc) The morpheme, which forms the center part of the world, is also called \"stem\". In English, a word can be made up of one or more morphemes, e.g.,\n\n\n\n\nword - thing           -\n stem \nthink\n\nword - localize        -\n stem \nlocal\n, suffix \nize\n\nword - denationalize   -\n prefix \nde\n, stem \nnation\n, suffix \nal\n, \nize\n\n\n\n\n\nThe computational tool to perform morphological parsing is finite state transducer. A transducer performs it by mapping between the two sets of symbols, and a finite state transducer does it with finite automaton. A transducer normally consists of four parts: \nrecognizer\n, \ngenerator\n,  \ntranslator\n, and \nrelator\n. The output of the transducer becomes a set of morphemes.\n\n\nLexical Analysis\n\n\nIn this phase of natural language analysis, validity of words according to lexicon is checked. Lexicon stands for dictionary. It is a collection of all possible valid words of language along with their meaning.\n\n\nIn NLP, the first stage of processing input text is to scan each word in sentence and compute (or look-up) all the relevant linguistic information about that word. The lexicon provides the necessary rules and data for carrying out the first stage analysis.\n\n\nThe details of words, like their type (noun, verb and adverb, and other details of nouns and verb, etc.) are checked.\n\n\n\n\nLexical analysis is dividing the whole chunk of text into paragraphs, sentences, and words.\n\n\nSyntactic Analysis\n\n\nSyntax refers to the study of formal relationships between words of sentences. In this phase the validity of a sentence according to grammar rules is checked. To perform the syntactic analysis, the knowledge of grammar and parsing is required. Grammar is formal specification of rules allowable in the language, and parsing is a method of analyzing a sentence to determine its structure according to grammar. The most common grammar used for syntactic analysis for natural languages are \ncontext free grammar\n (CFG) also called \nphase structure grammar\n and \ndefinite clause grammar\n. These grammars are described in detail in a separate actions.\n\n\n\n\nSyntactic analysis is done using parsing. Two basic parsing techniques are: \ntop-down parsing\n and \nbottom-up parsing\n.\n\n\nSemantic Analysis\n\n\nIn linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings. It also involves removing features specific to particular linguistic and cultural contexts, to the extent that such a project is possible.\n\n\nThe elements of idiom and figurative speech, being cultural, are often also converted into relatively invariant meanings in semantic analysis. Semantics, although related to pragmatics, is distinct in that the former deals with word or sentence choice in any given context, while pragmatics considers the unique or particular meaning derived from context or tone. To reiterate in different terms, semantics is about universally coded meaning, and pragmatics the meaning encoded in words that is then interpreted by an audience\n\n\nDiscourse Analysis\n\n\nThe meaning of any sentence depends upon the meaning of the sentence just before it. In addition, it also brings about the meaning of immediately succeeding sentence.\n\n\nTopics of discourse analysis include:\n\n\n\n\nThe various levels or dimensions of discourse, such as sounds, gestures, syntrax, the lexicon, style, rhetoric, meanings, speech acts, moves, strategies, turns, and other aspects of interaction\n\n\nGenres of discourse (various types of discourse in politics, the media, education, science, business, etc.)\n\n\nThe relations between text (discourse) and context\n\n\nThe relations between discourse and power\n\n\nThe relations between discourse and interaction\n\n\nThe relations between discourse and cognition and memory\n\n\n\n\nPragmatic Analysis\n\n\nDuring this, what was said is re-interpreted on what it actually meant. It involves deriving those aspects of language which require real world knowledge.\n\n\nSentiment Analysis\n\n\nMetaMind\n,\u00a0@RichardSocher\n\n\nNamed Entity Recognition\n\n\nKDD 2015 Tutorial: Automatic Entity Recognition and Typing from Massive Text Corpora - A Phrase and Network Mining Approach\n\n\nRelationship Extraction\n\n\nAlchemyAPI\n\n\nReferences\n\n\n\n\nKumar, Ela. \nNatural Language Processing\n. New Delhi: I.K. International Publishing House, 2011\n\n\n\"Artificial Intelligence Natural Language Processing\".\u00a0\nwww.tutorialspoint.com\n. N.p., 2016. Web. 11 Oct. 2016.", 
            "title": "Tasks"
        }, 
        {
            "location": "/task/#nlp-tasks", 
            "text": "The analysis of natural language is broken into various board levels such as phonological, morphological, syntactic, semantic, pragmatic and discourse analysis.", 
            "title": "NLP Tasks"
        }, 
        {
            "location": "/task/#phonological-analysis", 
            "text": "Phonology is analysis of spoken language. Therefore, it deals with speech recognition and generation. The core task of speech recognition and generation system is to take an acoustic waveform as input and produce as output, a string of words. The phonology is a part of natural language analysis, which deals with it. The area of computational linguistics that deals with speech analysis is computational phonology  Example:   Hans Rosling's shortest TED talk    Original Sound   \n   \nYour browser does not support the audio element.     Text  \nX means unknown but the world is pretty known it's seven billion people have seven stones. One billion can save money to fly abroad on holiday every year. One billion can save money to keep a car or buy a car. And then three billion they save money to pay the by be a bicycle or perhaps a two-wheeler. And two billion they are busy saving money to buy shoes. In the future they will get rich and these people we move over here, these people will move over here, we will have two billion more in the world like this and the question is whether the rich people over there are prepared to be integrated in the world with 10 bilions people.    Auto generated sound   \n   \nYour browser does not support the audio element.", 
            "title": "Phonological Analysis"
        }, 
        {
            "location": "/task/#morphological-analysis", 
            "text": "It is the most elementary phase of NLP. It deals with the word formation. In this phase, individual words are analyzed according to their components called  \"morphemes\" . In addition, non-word taken such as punctuation, etc. are separated from words. Morpheme is basic grammatical building block that makes words.   The study of word structure is refereed to as morphology. In natural language processing, it is done in morphological analysis. The task of breaking a word into its morphemes is called morphological parsing. A morpheme is defined as minimal meaningful unit in a language, which cannot be further broken into smaller units.  Example: word  fox  consists a single morpheme, as it cannot be further resolved into smaller units. Whereas word  cats  consists two morphemes, the morpheme \"cat\" and morpheme \"s\" indicating plurality.  Here we defined the term meaningful. Though cat can be broken in \"c\" and \"at\", but these do not relate with word \"cat\" in any sense. Thus word \"cat\" will be dealt with as minimum meaningful unit.  Morphemes are traditionally divided into two types   (i) \"free morphemes\", that are able to act as words in isolation (e.g., \"thing\", \"permanent\", \"local\")  (ii) \"bound morphemes\", that can operate only as part of other words (e.g., \"is\" 'ing' etc) The morpheme, which forms the center part of the world, is also called \"stem\". In English, a word can be made up of one or more morphemes, e.g.,   word - thing           -  stem  think \nword - localize        -  stem  local , suffix  ize \nword - denationalize   -  prefix  de , stem  nation , suffix  al ,  ize   The computational tool to perform morphological parsing is finite state transducer. A transducer performs it by mapping between the two sets of symbols, and a finite state transducer does it with finite automaton. A transducer normally consists of four parts:  recognizer ,  generator ,   translator , and  relator . The output of the transducer becomes a set of morphemes.", 
            "title": "Morphological Analysis"
        }, 
        {
            "location": "/task/#lexical-analysis", 
            "text": "In this phase of natural language analysis, validity of words according to lexicon is checked. Lexicon stands for dictionary. It is a collection of all possible valid words of language along with their meaning.  In NLP, the first stage of processing input text is to scan each word in sentence and compute (or look-up) all the relevant linguistic information about that word. The lexicon provides the necessary rules and data for carrying out the first stage analysis.  The details of words, like their type (noun, verb and adverb, and other details of nouns and verb, etc.) are checked.   Lexical analysis is dividing the whole chunk of text into paragraphs, sentences, and words.", 
            "title": "Lexical Analysis"
        }, 
        {
            "location": "/task/#syntactic-analysis", 
            "text": "Syntax refers to the study of formal relationships between words of sentences. In this phase the validity of a sentence according to grammar rules is checked. To perform the syntactic analysis, the knowledge of grammar and parsing is required. Grammar is formal specification of rules allowable in the language, and parsing is a method of analyzing a sentence to determine its structure according to grammar. The most common grammar used for syntactic analysis for natural languages are  context free grammar  (CFG) also called  phase structure grammar  and  definite clause grammar . These grammars are described in detail in a separate actions.   Syntactic analysis is done using parsing. Two basic parsing techniques are:  top-down parsing  and  bottom-up parsing .", 
            "title": "Syntactic Analysis"
        }, 
        {
            "location": "/task/#semantic-analysis", 
            "text": "In linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings. It also involves removing features specific to particular linguistic and cultural contexts, to the extent that such a project is possible.  The elements of idiom and figurative speech, being cultural, are often also converted into relatively invariant meanings in semantic analysis. Semantics, although related to pragmatics, is distinct in that the former deals with word or sentence choice in any given context, while pragmatics considers the unique or particular meaning derived from context or tone. To reiterate in different terms, semantics is about universally coded meaning, and pragmatics the meaning encoded in words that is then interpreted by an audience", 
            "title": "Semantic Analysis"
        }, 
        {
            "location": "/task/#discourse-analysis", 
            "text": "The meaning of any sentence depends upon the meaning of the sentence just before it. In addition, it also brings about the meaning of immediately succeeding sentence.  Topics of discourse analysis include:   The various levels or dimensions of discourse, such as sounds, gestures, syntrax, the lexicon, style, rhetoric, meanings, speech acts, moves, strategies, turns, and other aspects of interaction  Genres of discourse (various types of discourse in politics, the media, education, science, business, etc.)  The relations between text (discourse) and context  The relations between discourse and power  The relations between discourse and interaction  The relations between discourse and cognition and memory", 
            "title": "Discourse Analysis"
        }, 
        {
            "location": "/task/#pragmatic-analysis", 
            "text": "During this, what was said is re-interpreted on what it actually meant. It involves deriving those aspects of language which require real world knowledge.", 
            "title": "Pragmatic Analysis"
        }, 
        {
            "location": "/task/#sentiment-analysis", 
            "text": "MetaMind ,\u00a0@RichardSocher", 
            "title": "Sentiment Analysis"
        }, 
        {
            "location": "/task/#named-entity-recognition", 
            "text": "KDD 2015 Tutorial: Automatic Entity Recognition and Typing from Massive Text Corpora - A Phrase and Network Mining Approach", 
            "title": "Named Entity Recognition"
        }, 
        {
            "location": "/task/#relationship-extraction", 
            "text": "AlchemyAPI", 
            "title": "Relationship Extraction"
        }, 
        {
            "location": "/task/#references", 
            "text": "Kumar, Ela.  Natural Language Processing . New Delhi: I.K. International Publishing House, 2011  \"Artificial Intelligence Natural Language Processing\".\u00a0 www.tutorialspoint.com . N.p., 2016. Web. 11 Oct. 2016.", 
            "title": "References"
        }, 
        {
            "location": "/spelling_correction/", 
            "text": "Spelling Correction\n\n\nCorpus\n\n\nBirkbeck spelling error corpus\n\n\nReferences\n\n\n\n\nHow to Write a Spelling Corrector. \nPeter Norvig\n. 2007\n\n\nStatistical Natural Language Processing in Python. \nPeter Norvig\n. 2007", 
            "title": "Spelling Correction"
        }, 
        {
            "location": "/spelling_correction/#spelling-correction", 
            "text": "", 
            "title": "Spelling Correction"
        }, 
        {
            "location": "/spelling_correction/#corpus", 
            "text": "Birkbeck spelling error corpus", 
            "title": "Corpus"
        }, 
        {
            "location": "/spelling_correction/#references", 
            "text": "How to Write a Spelling Corrector.  Peter Norvig . 2007  Statistical Natural Language Processing in Python.  Peter Norvig . 2007", 
            "title": "References"
        }, 
        {
            "location": "/ner_crf/", 
            "text": "Conditional Random Fields in Name Entity Recognition\n\n\nIn this tutorial, I will write about how to using CRF++ to train your data for name entity recognition task.\n\n\nEnvironment:\n\n\n\n\nUbuntu 14.04\n\n\n\n\nInstall CRF++\n\n\n\n\n\n\nDownload CRF++-0.58.tar.gz\n\n\n\n\n\n\nExtact CRF++-0.58.tar.gz file\n\n\n\n\n\n\nNavigate to the location of extracted folder through\n\n\n\n\n\n\nInstall CRF++ from source\n\n\n\n\n\n\n./configure\nmake\nsudo make install\nldconfig\n\n\n\n\nCongratulations! CRF++ is install\n\n\ncrf_learn\n\n\n\n\nTraining CRF\n\n\nTo train a CRF using CRF++, you need 2 things:\n\n\n\n\nA template file: where you define features to be considered for training\n\n\nA training data file: where you have data in CoNLL format\n\n\n\n\ncrf_learn -t template_file train_data_file model\n\ncrf_learn -t template train.txt model\n\n\n\n\nA binary of model is produce.\n\n\nTo test this model, on a testing data\n\n\ncrf_test -m model testfile \n output.txt\n\ncrf_test -m model test.txt \n output.txt\n\n\n\n\nReferences\n\n\n\n\nConditional Random Fields : Installing CRF++ on Ubuntu\n\n\nConditional Random Fields Training and Testing using CRF++", 
            "title": "CRF in NER"
        }, 
        {
            "location": "/ner_crf/#conditional-random-fields-in-name-entity-recognition", 
            "text": "In this tutorial, I will write about how to using CRF++ to train your data for name entity recognition task.  Environment:   Ubuntu 14.04", 
            "title": "Conditional Random Fields in Name Entity Recognition"
        }, 
        {
            "location": "/ner_crf/#install-crf", 
            "text": "Download CRF++-0.58.tar.gz    Extact CRF++-0.58.tar.gz file    Navigate to the location of extracted folder through    Install CRF++ from source    ./configure\nmake\nsudo make install\nldconfig  Congratulations! CRF++ is install  crf_learn", 
            "title": "Install CRF++"
        }, 
        {
            "location": "/ner_crf/#training-crf", 
            "text": "To train a CRF using CRF++, you need 2 things:   A template file: where you define features to be considered for training  A training data file: where you have data in CoNLL format   crf_learn -t template_file train_data_file model\n\ncrf_learn -t template train.txt model  A binary of model is produce.  To test this model, on a testing data  crf_test -m model testfile   output.txt\n\ncrf_test -m model test.txt   output.txt", 
            "title": "Training CRF"
        }, 
        {
            "location": "/ner_crf/#references", 
            "text": "Conditional Random Fields : Installing CRF++ on Ubuntu  Conditional Random Fields Training and Testing using CRF++", 
            "title": "References"
        }, 
        {
            "location": "/entity_linking/", 
            "text": "Entity Linking\n\n\n\n\nIn natural language processing, \nentity linking\n, \nnamed entity linking (NEL)\n, \nnamed entity disambiguation (NED)\n, \nnamed entity recognition and disambiguation (NERD)\n or \nnamed entity normalization (NEN)\n is the task of determining the identity of entities mentioned in text. More precise, it is the task of linking entity mentions to entries in a knowledge base (e.g., DBpedia, Wikipedia)\n\n\nEntity linking requires a knowledge base containing the entities to which entity mentions can be linked. A popular choice for entity linking on open domain text are knowledge-bases based on Wikipedia, in which each page is regarded as a named entity. NED using Wikipedia entities has been also called wikification (see Wikify! an early entity linking system] ). A knowledge base may also be induced automatically from training text or manually built.\n\n\nNED is different from named entity recognition (NER) in that NER identifies the occurrence or mention of a named entity in text but it does not identify which specific entity it is\n\n\nExamples\n\n\nExample 1:\n\n\nFor example, given the sentence \"Paris is the capital of France\", the idea is to determine that \"Paris\" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred as \"Paris\".\n\n\n\n\nExample 2:\n\n\nGive the sentence \"In Second Debate, Donald Trump and HIllary Clinton Spar in Bitter, Personal Terms\", the idea is to determine that \"Donald Trump\" refer to an American politician, and \"Hillary Clinton\" refer to 67th United States Secretary of State from 2009 to 2013.\n\n\n\n\nArchitecture\n\n\n\n\n\n\nMention detection\n: Identification of text snippets that can potentially be linked to entities\n\n\nCandidate selection\n: Generating a set of candidate entities for each mention\n\n\nDisambiguation\n: Selecting a single entity (or none) for each mention, based on the context\n\n\n\n\nMention detection\n\n\n\n\nGoal: Detect all \"linkable\" phrases\n\n\nChallenges:\n\n\n\n\nRecall oriented: Do not miss any entity that should be link\n\n\nFind entity name variants (e.g. \"jlo\" is name variant of [Jennifer Lopez])\n\n\nFilter out inappropriate ones (e.g. \"new york\" matches \n2k different entities)\n\n\n\n\nCommon Approach\n\n\n\n\n\n\nBuild a dictionary of entity surface forms\n\n\n\n\n\n\nentities with all names variants\n\n\n\n\n\n\nCheck all document n-grams against the dictionary\n\n\n\n\n\n\nthe value of n is set typically between 6 and 8\n\n\n\n\n\n\nFilter out undesired entities\n\n\n\n\n\n\nCan be done here or later in the pipeline\n\n\n\n\n\n\nExamples\n\n\n\n\nCandidate Selection\n\n\n\n\nGoal: Narrow down the space of disambiguation possibilities\n\n\nBalances between precision and recall (effectiveness vs. efficiency)\n\n\nOften approached as ranking problem: keeping only candidates above a score/rank threshold for downstream processing.\n\n\nCommonness\n\n\nPerform the ranking of candidate entities based on their overall popularity, i.e., \"most command sense\"\n\n\n\n\nExamples\n\n\n\n\nCommonness can be pre-computed and stored in the entity surface form dictionary. Follows a power law with a long tail of extremely unlikely senses; entities at the tail end of distribution can be safely discarded (e.g., 0.001 is sensible threshold)\n\n\n\n\nDisambiguation\n\n\n\n\nBaseline approach: most common sense\n\n\nConsider additional types of evidence: \nprior importance\n of entities and mentions, \ncontextual similarity\n between the text surrounding the mention and the candidate entity, \ncoherence\n among all entity linking decisions in the document.\n\n\nCombine these signals: using supervised learning or graph-based approaches\n\n\nOptionally perform pruning: reject low confidence or semantically meaning less annotations.\n\n\nReferences\n\n\n\n\n\"Entity Linking\". \nwikipedia\n\n\n\"Entity Linking\". \nKrisztian Balog, University of Stavanger, 10th Russian Summer School in Information Retrieval\n. 2016\n\n\n\"An End-to-End Entity Linking Approach for Tweets\". \nIkuya Yamada, Hideaki Takeda, Yoshiyasu Takefuji\n. 2015", 
            "title": "Entity Linking"
        }, 
        {
            "location": "/entity_linking/#entity-linking", 
            "text": "In natural language processing,  entity linking ,  named entity linking (NEL) ,  named entity disambiguation (NED) ,  named entity recognition and disambiguation (NERD)  or  named entity normalization (NEN)  is the task of determining the identity of entities mentioned in text. More precise, it is the task of linking entity mentions to entries in a knowledge base (e.g., DBpedia, Wikipedia)  Entity linking requires a knowledge base containing the entities to which entity mentions can be linked. A popular choice for entity linking on open domain text are knowledge-bases based on Wikipedia, in which each page is regarded as a named entity. NED using Wikipedia entities has been also called wikification (see Wikify! an early entity linking system] ). A knowledge base may also be induced automatically from training text or manually built.  NED is different from named entity recognition (NER) in that NER identifies the occurrence or mention of a named entity in text but it does not identify which specific entity it is", 
            "title": "Entity Linking"
        }, 
        {
            "location": "/entity_linking/#examples", 
            "text": "Example 1:  For example, given the sentence \"Paris is the capital of France\", the idea is to determine that \"Paris\" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred as \"Paris\".   Example 2:  Give the sentence \"In Second Debate, Donald Trump and HIllary Clinton Spar in Bitter, Personal Terms\", the idea is to determine that \"Donald Trump\" refer to an American politician, and \"Hillary Clinton\" refer to 67th United States Secretary of State from 2009 to 2013.", 
            "title": "Examples"
        }, 
        {
            "location": "/entity_linking/#architecture", 
            "text": "Mention detection : Identification of text snippets that can potentially be linked to entities  Candidate selection : Generating a set of candidate entities for each mention  Disambiguation : Selecting a single entity (or none) for each mention, based on the context", 
            "title": "Architecture"
        }, 
        {
            "location": "/entity_linking/#mention-detection", 
            "text": "Goal: Detect all \"linkable\" phrases  Challenges:   Recall oriented: Do not miss any entity that should be link  Find entity name variants (e.g. \"jlo\" is name variant of [Jennifer Lopez])  Filter out inappropriate ones (e.g. \"new york\" matches  2k different entities)", 
            "title": "Mention detection"
        }, 
        {
            "location": "/entity_linking/#common-approach", 
            "text": "Build a dictionary of entity surface forms    entities with all names variants    Check all document n-grams against the dictionary    the value of n is set typically between 6 and 8    Filter out undesired entities    Can be done here or later in the pipeline    Examples", 
            "title": "Common Approach"
        }, 
        {
            "location": "/entity_linking/#candidate-selection", 
            "text": "Goal: Narrow down the space of disambiguation possibilities  Balances between precision and recall (effectiveness vs. efficiency)  Often approached as ranking problem: keeping only candidates above a score/rank threshold for downstream processing.", 
            "title": "Candidate Selection"
        }, 
        {
            "location": "/entity_linking/#commonness", 
            "text": "Perform the ranking of candidate entities based on their overall popularity, i.e., \"most command sense\"   Examples   Commonness can be pre-computed and stored in the entity surface form dictionary. Follows a power law with a long tail of extremely unlikely senses; entities at the tail end of distribution can be safely discarded (e.g., 0.001 is sensible threshold)", 
            "title": "Commonness"
        }, 
        {
            "location": "/entity_linking/#disambiguation", 
            "text": "Baseline approach: most common sense  Consider additional types of evidence:  prior importance  of entities and mentions,  contextual similarity  between the text surrounding the mention and the candidate entity,  coherence  among all entity linking decisions in the document.  Combine these signals: using supervised learning or graph-based approaches  Optionally perform pruning: reject low confidence or semantically meaning less annotations.", 
            "title": "Disambiguation"
        }, 
        {
            "location": "/entity_linking/#references", 
            "text": "\"Entity Linking\".  wikipedia  \"Entity Linking\".  Krisztian Balog, University of Stavanger, 10th Russian Summer School in Information Retrieval . 2016  \"An End-to-End Entity Linking Approach for Tweets\".  Ikuya Yamada, Hideaki Takeda, Yoshiyasu Takefuji . 2015", 
            "title": "References"
        }, 
        {
            "location": "/application/", 
            "text": "NLP Applications\n\n\nInformation Retrieval (IR)\n\n\nInformation retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.\n\n\nInformation Extraction (IE)\n\n\nInformation extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP).\n\n\nMachine Translation\n\n\nMachine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n\n\nQuestion Answering (QA)\n\n\nQuestion answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.", 
            "title": "Applications"
        }, 
        {
            "location": "/application/#nlp-applications", 
            "text": "", 
            "title": "NLP Applications"
        }, 
        {
            "location": "/application/#information-retrieval-ir", 
            "text": "Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.", 
            "title": "Information Retrieval (IR)"
        }, 
        {
            "location": "/application/#information-extraction-ie", 
            "text": "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP).", 
            "title": "Information Extraction (IE)"
        }, 
        {
            "location": "/application/#machine-translation", 
            "text": "Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.", 
            "title": "Machine Translation"
        }, 
        {
            "location": "/application/#question-answering-qa", 
            "text": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.", 
            "title": "Question Answering (QA)"
        }, 
        {
            "location": "/vietnlp/", 
            "text": "Vietnamese NLP\n\n\n\n\n\n\n\n\nCore Problems\n\n\nDictionaries\n\n\n\n\n2004, H\u1ed3 Ng\u1ecdc \u0110\u1ee9c, The Free Vietnamese Dictionary Project\n\n\n\n\nWordnet\n\n\n\n\nviet wordnet\n\n\n\n\nCorpus\n\n\nVNESEcorpus\n, 650.000 sentences, 10.000 articles from vietnamnet.vn, dantri.com.vn, nhanhdan.com.vn. Size: 64.59 Mb\n\n\nVNTQcorpus(small)\n, 300.000 sentences, 1.000 articles from vnthuquan.net\nSize: ~35 Mb\n\n\nVNTQcorpus(big)\n, 1.750.000 sentences, 13.000 articles from vnthuquan.net, Size: ~240 Mb\n\n\nWord Segmentation\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: F1 (%)\n\n\n\n\n\n\n\n\nPOS Tagging\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: F1 (%)\n\n\n\n\n\n\n\n\nRelated Readings\n\n\n\n\n\n\n\n\nCoreference\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: %\n\n\n\n\n\n\n\n\nRelated Readings\n\n\n\n\n\n\n\n\nDependency Parsing\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: %\n\n\n\n\n\n\n\n\nRelated Readings\n\n\n\n\n\n\n\n\nChunking\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: %\n\n\n\n\n\n\n\n\nSpelling Correction\n\n\nBenchmark\n\n\n\n\n\n\n\n\nNamed Entity Recognition\n\n\nsources \n1\n\n\nBenchmark\n\n\nUnit: %\n\n\n\n\n\n\n\n\nRelated Readings\n\n\n\n\n\n\n\n\nGroups and People\n\n\nGroups\n\n\n\n\nvnlp.net, (2010-now)\n\n\nkde lab, (2014-now)\n\n\n\n\nPeople\n\n\n\n\nAssoc. Prof. Ha Quang Thuy\n\n\nProf. Tu-Bao Ho\n\n\nKhoat Than\n\n\nCam Tu Nguyen\n\n\nCAM-TU NGUYEN\n\n\nLe Hong Phuong\n\n\nPhan Xuan Hieu\n\n\nTr\u1ea7n Mai V\u0169\n\n\nNguy\u1ec5n Ki\u00eam Hi\u1ebfu\n\n\n\n\nApplications\n\n\nVAV - Tr\u1ee3 l\u00fd \u1ea3o cho ng\u01b0\u1eddi Vi\u1ec7t\n\n\nDate: Nov 2015 - now\n\n\nMDN-Team, Khoa CNTT, Tr\u01b0\u1eddng \u0110H C\u00f4ng ngh\u1ec7, \u0110HQG HN Tools\n\n\nB\u1ea1n \u0111ang ngh\u0129 \u0111\u1ebfn m\u1ed9t \u1ee9ng d\u1ee5ng th\u00f4ng minh tr\u00ean di \u0111\u1ed9ng cho ph\u00e9p b\u1ea1n t\u01b0\u01a1ng t\u00e1c b\u1eb1ng gi\u1ecdng n\u00f3i \u0111\u1ec3 h\u1eb9n chu\u00f4ng b\u00e1o th\u1ee9c, \u0111\u1eb7t l\u1ecbch cho m\u1ed9t cu\u1ed9c h\u1ecdp, b\u1eadt \u0111\u1ecbnh v\u1ecb, g\u1ecdi \u0111i\u1ec7n cho ai \u0111\u00f3, truy c\u1eadp m\u1ed9t trang web b\u1ea5t k\u1ef3, t\u00ecm \u0111\u01b0\u1eddng tr\u00ean b\u1ea3n \u0111\u1ed3, \u0111\u1ecbnh v\u1ecb c\u00e2y ATM c\u1ee7a m\u1ed9t ng\u00e2n h\u00e0ng n\u00e0o \u0111\u00f3 g\u1ea7n v\u1edbi b\u1ea1n, hay th\u01b0\u1edfng th\u1ee9c m\u1ed9t b\u1ea3n nh\u1ea1c m\u00ecnh y\u00eau th\u00edch \u2026 \u1ee8ng d\u1ee5ng Tr\u1ee3 l\u00fd \u1ea3o VAV ch\u00ednh l\u00e0 c\u00e2u tr\u1ea3 l\u1eddi cho b\u1ea1n. \u0110\u01b0\u1ee3c thi\u1ebft k\u1ebf v\u00e0 ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean c\u00e1c k\u1ef9 thu\u1eadt tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o (h\u1ecdc m\u00e1y, ph\u00e2n t\u00edch v\u00e0 hi\u1ec3u ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean), VAV c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c \u00fd \u0111\u1ecbnh c\u1ee7a b\u1ea1n d\u00f9 b\u1ea1n di\u1ec5n \u0111\u1ea1t c\u00e2u l\u1ec7nh c\u1ee7a m\u00ecnh theo nhi\u1ec1u c\u00e1ch kh\u00e1c nhau m\u00e0 kh\u00f4ng c\u1ea7n tu\u00e2n theo b\u1ea5t k\u1ef3 khu\u00f4n m\u1eabu n\u00e0o cho tr\u01b0\u1edbc. Nh\u1eefng g\u00ec VAV h\u01b0\u1edbng t\u1edbi l\u00e0 tr\u1edf th\u00e0nh m\u1ed9t tr\u1ee3 l\u00fd \u1ea3o th\u00f4ng minh gi\u00fap b\u1ea1n th\u1ef1c hi\u1ec7n nh\u1eefng \u0111i\u1ec1u m\u00ecnh mu\u1ed1n v\u00e0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u1ed3ng h\u00e0nh th\u00e2n thi\u1ec7n, d\u00ed d\u1ecfm b\u00ean b\u1ea1n.\n\n\nReferences\n\n\n\n\n\n\n\n\n\n\n2016, Big Challenges for Data Scientists at VCCORP", 
            "title": "Overview"
        }, 
        {
            "location": "/vietnlp/#vietnamese-nlp", 
            "text": "", 
            "title": "Vietnamese NLP"
        }, 
        {
            "location": "/vietnlp/#core-problems", 
            "text": "", 
            "title": "Core Problems"
        }, 
        {
            "location": "/vietnlp/#dictionaries", 
            "text": "2004, H\u1ed3 Ng\u1ecdc \u0110\u1ee9c, The Free Vietnamese Dictionary Project", 
            "title": "Dictionaries"
        }, 
        {
            "location": "/vietnlp/#wordnet", 
            "text": "viet wordnet", 
            "title": "Wordnet"
        }, 
        {
            "location": "/vietnlp/#corpus", 
            "text": "VNESEcorpus , 650.000 sentences, 10.000 articles from vietnamnet.vn, dantri.com.vn, nhanhdan.com.vn. Size: 64.59 Mb  VNTQcorpus(small) , 300.000 sentences, 1.000 articles from vnthuquan.net\nSize: ~35 Mb  VNTQcorpus(big) , 1.750.000 sentences, 13.000 articles from vnthuquan.net, Size: ~240 Mb", 
            "title": "Corpus"
        }, 
        {
            "location": "/vietnlp/#word-segmentation", 
            "text": "sources  1", 
            "title": "Word Segmentation"
        }, 
        {
            "location": "/vietnlp/#benchmark", 
            "text": "Unit: F1 (%)", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#pos-tagging", 
            "text": "sources  1", 
            "title": "POS Tagging"
        }, 
        {
            "location": "/vietnlp/#benchmark_1", 
            "text": "Unit: F1 (%)", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#related-readings", 
            "text": "", 
            "title": "Related Readings"
        }, 
        {
            "location": "/vietnlp/#coreference", 
            "text": "sources  1", 
            "title": "Coreference"
        }, 
        {
            "location": "/vietnlp/#benchmark_2", 
            "text": "Unit: %", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#related-readings_1", 
            "text": "", 
            "title": "Related Readings"
        }, 
        {
            "location": "/vietnlp/#dependency-parsing", 
            "text": "sources  1", 
            "title": "Dependency Parsing"
        }, 
        {
            "location": "/vietnlp/#benchmark_3", 
            "text": "Unit: %", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#related-readings_2", 
            "text": "", 
            "title": "Related Readings"
        }, 
        {
            "location": "/vietnlp/#chunking", 
            "text": "sources  1", 
            "title": "Chunking"
        }, 
        {
            "location": "/vietnlp/#benchmark_4", 
            "text": "Unit: %", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#spelling-correction", 
            "text": "", 
            "title": "Spelling Correction"
        }, 
        {
            "location": "/vietnlp/#benchmark_5", 
            "text": "", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#named-entity-recognition", 
            "text": "sources  1", 
            "title": "Named Entity Recognition"
        }, 
        {
            "location": "/vietnlp/#benchmark_6", 
            "text": "Unit: %", 
            "title": "Benchmark"
        }, 
        {
            "location": "/vietnlp/#related-readings_3", 
            "text": "", 
            "title": "Related Readings"
        }, 
        {
            "location": "/vietnlp/#groups-and-people", 
            "text": "Groups   vnlp.net, (2010-now)  kde lab, (2014-now)   People   Assoc. Prof. Ha Quang Thuy  Prof. Tu-Bao Ho  Khoat Than  Cam Tu Nguyen  CAM-TU NGUYEN  Le Hong Phuong  Phan Xuan Hieu  Tr\u1ea7n Mai V\u0169  Nguy\u1ec5n Ki\u00eam Hi\u1ebfu", 
            "title": "Groups and People"
        }, 
        {
            "location": "/vietnlp/#applications", 
            "text": "VAV - Tr\u1ee3 l\u00fd \u1ea3o cho ng\u01b0\u1eddi Vi\u1ec7t  Date: Nov 2015 - now  MDN-Team, Khoa CNTT, Tr\u01b0\u1eddng \u0110H C\u00f4ng ngh\u1ec7, \u0110HQG HN Tools  B\u1ea1n \u0111ang ngh\u0129 \u0111\u1ebfn m\u1ed9t \u1ee9ng d\u1ee5ng th\u00f4ng minh tr\u00ean di \u0111\u1ed9ng cho ph\u00e9p b\u1ea1n t\u01b0\u01a1ng t\u00e1c b\u1eb1ng gi\u1ecdng n\u00f3i \u0111\u1ec3 h\u1eb9n chu\u00f4ng b\u00e1o th\u1ee9c, \u0111\u1eb7t l\u1ecbch cho m\u1ed9t cu\u1ed9c h\u1ecdp, b\u1eadt \u0111\u1ecbnh v\u1ecb, g\u1ecdi \u0111i\u1ec7n cho ai \u0111\u00f3, truy c\u1eadp m\u1ed9t trang web b\u1ea5t k\u1ef3, t\u00ecm \u0111\u01b0\u1eddng tr\u00ean b\u1ea3n \u0111\u1ed3, \u0111\u1ecbnh v\u1ecb c\u00e2y ATM c\u1ee7a m\u1ed9t ng\u00e2n h\u00e0ng n\u00e0o \u0111\u00f3 g\u1ea7n v\u1edbi b\u1ea1n, hay th\u01b0\u1edfng th\u1ee9c m\u1ed9t b\u1ea3n nh\u1ea1c m\u00ecnh y\u00eau th\u00edch \u2026 \u1ee8ng d\u1ee5ng Tr\u1ee3 l\u00fd \u1ea3o VAV ch\u00ednh l\u00e0 c\u00e2u tr\u1ea3 l\u1eddi cho b\u1ea1n. \u0110\u01b0\u1ee3c thi\u1ebft k\u1ebf v\u00e0 ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean c\u00e1c k\u1ef9 thu\u1eadt tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o (h\u1ecdc m\u00e1y, ph\u00e2n t\u00edch v\u00e0 hi\u1ec3u ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean), VAV c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c \u00fd \u0111\u1ecbnh c\u1ee7a b\u1ea1n d\u00f9 b\u1ea1n di\u1ec5n \u0111\u1ea1t c\u00e2u l\u1ec7nh c\u1ee7a m\u00ecnh theo nhi\u1ec1u c\u00e1ch kh\u00e1c nhau m\u00e0 kh\u00f4ng c\u1ea7n tu\u00e2n theo b\u1ea5t k\u1ef3 khu\u00f4n m\u1eabu n\u00e0o cho tr\u01b0\u1edbc. Nh\u1eefng g\u00ec VAV h\u01b0\u1edbng t\u1edbi l\u00e0 tr\u1edf th\u00e0nh m\u1ed9t tr\u1ee3 l\u00fd \u1ea3o th\u00f4ng minh gi\u00fap b\u1ea1n th\u1ef1c hi\u1ec7n nh\u1eefng \u0111i\u1ec1u m\u00ecnh mu\u1ed1n v\u00e0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u1ed3ng h\u00e0nh th\u00e2n thi\u1ec7n, d\u00ed d\u1ecfm b\u00ean b\u1ea1n.", 
            "title": "Applications"
        }, 
        {
            "location": "/vietnlp/#references", 
            "text": "2016, Big Challenges for Data Scientists at VCCORP", 
            "title": "References"
        }
    ]
}